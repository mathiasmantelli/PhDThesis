\chapter{Text as source of semantic information for visual object search in large and unknown environments}

\section{Introduction}
\label{introduction}
Humans perform several activities that are part of their routine in everyday living spaces, in which searching for objects or regions of interest (ROI) is one of them. It demands visual recognition and high-level reason as essential skills.~\cite{Dicarlo2012How}. In such a real situation, it is not prudent to assume that the desired object or ROI is always present in the human's field of view since the beginning. The environment navigation is another example of daily activities for humans, and it must be as intelligent as possible, even in unknown places~\cite{Talbot2016Find}. In such cases, humans must actively navigate and search for target objects in the environment, relying on their visual recognition abilities~\cite{Dicarlo2012How,Sjoo2012Topological}. 

Autonomous robots, which perform tasks related to object searching, such as home assistance, delivery of packages, manipulation in factories, and fetch and carry, also need to fulfill the same visual recognition and navigation requirements as humans. The progress concerning the tasks performed by autonomous robots is possible thanks to the late advances in the field of mobile robotics, more specifically in the localization, mapping, navigation, perception, and exploration ones~\cite{Aydemir2013Active}. Similar to humans in the context of object searching tasks, robots should also not rely on the assumption that objects are already within their field of view. Hence, they have to find objects in large-scale environments based on primarily visual sensors, which is known as visual object search (OS) problem~\cite{Sjoo2012Topological,Aydemir2013Active}. Briefly, the OS problem computes how to bring the target object into the mobile robot's sensor field of view. 

In the context of OS problems, the search strategy is one of the critical factors since it directly impacts the efficiency of a solution~\cite{Aydemir2013Active}. It is responsible for maximizing the probability of detecting the target object in the environment and minimizing the total cost of the task~\cite{Sjoo2012Topological}. There are different approaches to measure this cost, and the most common options are the time or distance traveled, and the robot's movements (each type of movement has a different value). For example, imagine that a courier robot delivers a package to a specific room within a large-scale and unknown building. The most straightforward search strategy would be the robot visiting all places in the environment and visually checking whether every new room is the target one. Even though many robot sensors have a limited field of view, it is very inefficient and time-consuming to make the robot visit all the existing rooms of a building to accomplish its task. In contrast to this simple strategy, semantic information about the environment could be gathered and used by a more efficient planner. Therefore, instead of making the robot exhaustively visit all the existing rooms to deliver the package to the target room, the search strategy would reason over the semantic information and infer important cues to improve the searching.

The research community has proposed essential and valuable works related to the OS problem~\cite{Sjoo2012Topological,Aydemir2013Active,Ekvall2007Object,Sjoo2009Object,Rasouli2020Attention}. However, despite these contributions, the problem is proven to be NP-Complete~\cite{Tsotsos1992Onthe,Ye2001AComplexity}. Then, the optimal search solution can be computed by approximation~\cite{Sjoo2012Topological}, minimizing the search cost as much as possible. In the example of the courier robot previously introduced, taking advantage of semantic information of the environment provides search cues for this approximation. Then, the robot should be able to reason over their sensor readings, infer supplementary knowledge, and increase their level of abstraction of the environment over time \cite{Barber2018Mobile}.

However, making the robot get such information from real scenarios, i.e. unexplored environments, includes additional cost and increases the difficulty level of OS systems~\cite{Aydemir2013Active}. The challenge of such systems relies on balancing exploration and knowledge exploitation, i.e. should the robot explore further or search for the target in the already known regions. This chapter considers the scenario where a courier robot delivers a package to a target room in a large-scale and unknown environment, with the shortest possible distance traveled. The searching strategy relies on numbers visually extracted from door signs in the environment to accomplish the task, as any information is provided to the robot beforehand.

\subsection{Proposal and contributions}

In this chapter, we present a semantic OS system that efficiently searches for objects, which in our case, the target is represented by the number of door signs, in large-scale and unknown buildings. We evaluate our semantic OS system in four different simulated indoor environments and one physical test as proof-of-concept implementation in which a robot is tasked with finding a target door sign. Besides, we compare the performance of our system with a purely geometric OS system called Greedy and humans teleoperating the robot to perform the same search task.

Different OS approaches have been proposed by the research community as earlier presented. In many of them, the robot is tasked with finding objects based on their visual appearance or position in the environment (e.g. a plate on the kitchen table). However, to the best of our knowledge, OS systems that search for door signs (not areas in general) and take numbers (in text format) as input to their strategy are not well explored yet. 

The novelties presented in this chapter in comparison to the already published systems are threefold: 
\begin{itemize}
	\item \textbf{Target object:} The target of our proposed semantic-based OS system is a specific door sign (called \textit{goal-door} in this chapter) within an unknown indoor environment, instead of an ROI, chair, table, or kitchen utensil like others works. This novelty enables, for example, autonomous courier robots to perform the final part of the delivery task, which happens after the robot arrives at the buildings, where there is no map available. Our system is relevant in moments such as the COVID-19 pandemic, in which people are recommended to stay at home and avoid social contact. Hence, it stimulates the use of fully autonomous robots for performing the entire delivery task. 
	\item \textbf{Textual information:} Our semantic OS system relies on textual information as a visual cue and more specific numbers extracted from the door signs. Large buildings, for instance, are divided into many small rooms, and usually comply with a pattern of signing each room~\cite{Aydemir2012What,Stanford2017Room,University2012Room,Brian2014Abasic}. Using numbers is different from considering the size of an ROI, or the features and colors of an object, for instance. If we could analyze humans while looking for a door sign in an unknown environment, most of them would try to figure out the door signing pattern. They would avoid exploring the entire building by analyzing how the door signs are related to each other to infer whether the current corridor is promising in terms of containing the goal-door. A courier robot could behave in a similar way to efficiently perform the searching task. Hence, our OS system processes the numbers to infer semantic information from them. The inferred information is the odd and even characteristics and whether the sequence of numbers is increasing or decreasing. In addition to the semantic information, our system also considers geometric information, which is the distance between the robot's pose and the unknown regions, and the history of the robot's orientation.
	\item \textbf{Advantages of semantic information:} we also present an analysis of the advantage of using text from the door signs and its inferred semantic information as input to our semantic OS system, instead of limiting it only to geometric information. The combination of this semantic information, inferred from the numbers, and the geometric information, extracted from the environment, are useful inputs for the reasoning of our system. It permits the efficient computation of search and exploration steps, guiding the robot towards areas more likely to lead to the target room. Lastly, in addition to this computation, which is fully probabilistic, our system also builds a 2D map of the environment. It is segmented based on spatial density information, i.e. according to different sizes of free space~\cite{Maffei2015Using}. Our system considers the laser readings to build the 2D map, and the images of an RGB camera to extract the numbers from the door signs. The numbers are used to infer semantic information and search cues, whereas the map indicates new regions to explore, and when combined, they indicate which direction is more likely to contain the goal-door to guide the robot.
\end{itemize}

%The remainder of this chapter is organized as follows. After reviewing the literature in Section \ref{sec:relatedwork}, Section \ref{sec:method} describes our semantic OS system and its basic components. Section \ref{sec:semanticPlanner} explains our semantic planner and how it considers the door signs as exploration cues to reach the goal-door. Next, Section \ref{sec:experimentsResults} introduces the experimental setup, and then it compares the results of our semantic OS system to, first, a purely geometric and coverage-based OS system called Greedy, and second to human participants performing the same task using the robot embodiment (teleoperating the robot and observing its sensor readings in the simulation setup). Lastly, it presents the results of our semantic OS system in a proof-of-concept that uses a physical robot performing the search in an unknown environment. The chapter conclusion is presented in Section \ref{sec:conclusion}, discussing the demonstrated outcomes.

\section{Semantic-based AVS system}
\label{sec:method}
This section details the basic modules that compose our semantic OS system. It starts with an overview of the system in section~\ref{subsec:overview}. Then it goes to the Mapping module in Section~\ref{subsec:mapping}, to explain how the 2D grid map is built. The Map Segmentation module is introduced in section~\ref{subsec:mapsegmentation}, which presents how the map is split into different segments. Finally, Section~\ref{subsec:imageprocessing} describes how the numbers are extracted from the door signs through computer vision algorithms. 

\subsection{Overview}
\label{subsec:overview}
Our proposed system performs OS, i.e. it guides the robot through an unknown environment until the robot reaches a specific location that contains the target object or finds it on the way. Our system focuses on indoor environments, such as buildings with many rooms identified by door signs. One of its advantages is that it does not require any a-priory knowledge about the environment, such as the door signs arrangement or where the robot should be heading. In our paper, the target to be found is a door sign (called here as goal-door), which is identified by a number. An example to illustrate the usage of our system is a courier robot that delivers a package. From the restaurant until the destination building, it uses Google Maps and its embedded GPS to navigate through the city. However, once it is inside the building, it does not have a map to plan its path through the corridors. Therefore, the robot has to search the target goal-door to deliver the package to the customer.

Our system is composed of four modules: Mapping, Image Processing, Map Segmentation and Semantic Planner. The Semantic Planner module, presented in Section \ref{sec:semanticPlanner}, is the main contribution of our paper. It requires a base system to work, composed by the first three other modules, that are discussed in Sections \ref{subsec:mapping}, \ref{subsec:mapsegmentation}, \ref{subsec:imageprocessing}. The first of these three modules, Mapping, aims to build a 2D grid map of the environment using the Histogramic In-Motion Mapping (HIMM) technique~\cite{Borenstein1991Histogramic}, that takes as input the readings from a \ang{180} laser sensor. The next module, Image Processing, processes the images taken by two RGB cameras, and it analyses them to recognise the number from door signs. Once identified, the module includes them into the 2D grid map at their respective side, i.e. left or right wall. The third module, Map Segmentation, is responsible for segmenting the free space of the 2D grid map according to its size using the Kernel Density Estimation (KDE) approach introduced by Maffei~\textit{et al.} \cite{Maffei2015Using}. This module also assigns to each segment of a corridor its respective list of door signs, that is the list of numbers recognised while the robot was within the corridor. The last module, Semantic Planner, calculates which path is more likely to contain the goal-door given its detected doors. The Boundary Value Problem (BVP) \cite{Prestes2002Exploration}, calculated over the grid map and the Voronoi diagram \cite{Guo1989Parallel}, moves the robot towards the path that is most attractive, defined by the Semantic Planner.

\subsection{Mapping module}
\label{subsec:mapping} %subsec:systembasis
As the robot, equipped with a laser range-finder, moves through the environment, it reads a set of measurements that are used as input to the HIMM method in the Mapping module. It aims to build a 2D occupancy grid map $\bs{m}$ of the environment, Fig. \ref{fig:2dmapSegmentation_a}. Over $\bs{m}$, the Mapping module also computes the Voronoi diagram to have the centre cells of the free spaces, represented by the green lines in Fig. \ref{fig:2dmapSegmentation_b}. The yellow region represents the free space that was visited by the robot kernel, i.e. the circle centred at robot's pose. Based on that, the BVP smoothly moves the robot through the environment, avoiding obstacles and keeping it as close as possible to the Voronoi cells. 

%\begin{figure}[h]
%\centering
%(a) \hspace{5cm} (b)\\
%\includegraphics[width=0.46\textwidth]{figs/1.jpeg}
%\includegraphics[width=0.46\textwidth]{figs/2.jpeg}\\
%\vspace{.2cm}
%(c) \hspace{5cm} (d)\\
%\includegraphics[width=0.46\textwidth]{figs/3.jpeg}
%\includegraphics[width=0.46\textwidth]{figs/4.jpeg}
%\caption{Example of our mapping and segmentation modules. (a) shows the white area representing the free cells and the pink line the robot path, (b) the marked yellow areas representing the visited cells considered by the kernel of Eq. \ref{eq:kerneldefinition}, and the Voronoi through the green lines, (c) our KDE-based module segmenting the visited cells of (b) as two types, and (d) the segment identification using different colours.}
%\label{fig:2dmapSegmentation}
%\end{figure}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/1.jpeg}
         \caption{}
         \label{fig:2dmapSegmentation_a}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/2.jpeg}
         \caption{}
         \label{fig:2dmapSegmentation_b}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/3.jpeg}
         \caption{}
         \label{fig:2dmapSegmentation_c}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/4.jpeg}
         \caption{}
         \label{fig:2dmapSegmentation_d}
     \end{subfigure}
     \caption{\small Example of our mapping and segmentation modules. (a) shows the white area representing the free cells and the pink line the robot path, (b) the marked yellow areas representing the visited cells considered by the kernel of Eq. \ref{eq:kerneldefinition}, and the Voronoi through the green lines, (c) our KDE-based module segmenting the visited cells of (b) as two types, and (d) the segment identification using different colours.}
     \label{fig:2dmapSegmentation}
 \end{figure}


\subsection{Map Segmentation module}
\label{subsec:mapsegmentation}
The Segmentation and Mapping modules are executed simultaneously, aiming to split the free space of $\bs{m}$ into multiple regions according to the size of free areas. Every segmented region is called a segment, and it is used to store important information from this group of cells. In $\bs{m}$, there might be different types of segments. Fig. \ref{fig:2dmapSegmentation_c} illustrates the case in which the Segmentation module considers only two types. In this case, it means that all green segments, or all the red ones, have a similar size of free space computed using the KDE. Besides their type, each segment is also singularly identified, such as $\mathbf{s}_i$ for the $i$-th segment within $\bs{m}$. Fig.~\ref{fig:2dmapSegmentation_d} shows the segmented 2D map as if each segment was identified as one colour.  

For this purpose, the Segmentation module uses the KDE approach~\cite{Maffei2015Using}. The $K(\cdot)$ is a uniform kernel that computes the size of the free area covered by it, defined as
\begin{equation}
    K(d) = \begin{cases}
a & \text{, if $d \leq r$} \\ 
0 & \text{, otherwise,} 
\end{cases}
\label{eq:uniformKernel}
\end{equation}
where $r$ is the radius and $a$ is the height of $K(\cdot)$, and $d$ is the Manhattan distance from the current cell being measured, $\mathbf{c} \in \mathbf{T}$, to the centre of the kernel, cell $\mathbf{c}_k \in \bs{m}$. $\mathbf{T} \in \bs{m}$ is a subset of cells that have been within the area of the kernel in any moment. For a given cell $\mathbf{c}$, $Q(\cdot)$ tests whether it is free, and it is defined as
\begin{equation}
\label{eq:freeCell}
    Q(\mathbf{c}) = 
\begin{cases}
1 & \text{, if $\mathbf{c}$ is a free cell} \\ 
0 & \text{, otherwise.} 
\end{cases}
\end{equation}

Combining the previous function into the KDE approach, it is possible to calculate the kernel density. For a cell $\mathbf{c}_k$, its free space $\Psi(\cdot)$ is computed by
\begin{equation}
\label{eq:kerneldefinition}
    \Psi(\mathbf{c}_k) = \sum_\mathbf{c}^\mathbf{T} Q(\mathbf{c})K(\left \| \mathbf{c} - \mathbf{c}_k \right \|).
\end{equation}
%The algorithm, in practice, is working over $\mathbf{M}$. Therefore, $\Psi$ is defined as a sum instead of an integral. 

According to Eq. \ref{eq:freeCell}, when unknown cells are found within the kernel area, it is still possible to compute $\Psi(\cdot)$. Therefore, once unknown cells return zero from Eq. \ref{eq:freeCell}, $\Psi(\cdot)$ can differentiate density measures when obstacles are surrounding $\mathbf{c}$ and decrease the size of the area computed by the kernel.

Assuming that the Segmentation module considers different sizes of free areas, and given that Eq. \ref{eq:kerneldefinition} calculates the size of free area surrounding a cell $\mathbf{c}_k \in \bs{m}$, $\Psi(\cdot)$ can be used in the Segmentation module as
\begin{equation}
\label{eq:corridorNotcorridor}
    \Upsilon(\textbf{c}_k) = \left \lfloor  \Psi(\textbf{c}_k) / \delta  \right \rfloor
\end{equation}
where $\delta$ is a threshold that defines how many different sizes of free areas are considered by the segmentation function, $\Upsilon(\cdot)$. %For instance, if $0 \leq \Psi(\mathbf{c}_k) \leq 800$ and $\delta = 400$, Eq. \ref{eq:corridorNotcorridor} segments free areas in only two different sizes, as illustrated by Fig. \ref{fig:2dmapSegmentation}-(b). On the other hand, if $\delta = 200$, it segments in four sizes. 
Therefore, a high $\delta$ means Eq. \ref{eq:corridorNotcorridor} considers few different sizes, whereas a low $\delta$ is the opposite. 

A segment $\mathbf{s}$ represents a group of free and adjacent cells from $\bs{m}$ that have the same $\Upsilon(\cdot)$. Fig. \ref{fig:2dmapSegmentation_d} demonstrates different segments, in which each one has a different colour. For example, given $\mathbf{c}_0$ and $\mathbf{c}_1$ as two free and neighbouring cells in $\bs{m}$, and that $\Upsilon(\mathbf{c}_0) = \Upsilon(\mathbf{c}_1)$, then both belong to the same segment $\mathbf{s}_0$. Otherwise, a new segment $\mathbf{s}_1$ is created and $\mathbf{c}_1$ is associated to it. Thus, the segmentation of free adjacent cells from $\bs{m}$ is based on Eq. \ref{eq:corridorNotcorridor}.

%\subsection{Image Processing module}
\label{subsec:imageprocessing}
The last module that completes the basis of our semantic AVS system is the Image Processing one. It aims to recognise the number of a door sign that may be in an RGB image. The idea here is to use one well known existing text recognition algorithm \cite{Zhang2013Text,Jung2004Text,Neumann2012Realtime}, since this is not the focus of our paper, and any approach can be used. The chosen work is the one proposed by \cite{Neumann2012Realtime} due to its real-time recognition aspect, and its robustness against noise and low contrast of characters. Besides, it does not require any information or preparation beforehand.

For a given image $\mathbf{I}$ that was captured by the robot at cell $\mathbf{c}$, for example Fig.~\ref{fig:imageProcessing}a and Fig.~\ref{fig:imageProcessing}c, the image processing module returns a list $\mathbf{L}$ containing the recognised number from door signs. In the case of Fig.~\ref{fig:imageProcessing}, $\mathbf{L}$ would contain only the number 228. Fig.~\ref{fig:imageProcessing} also shows where the detected door signs are included into the 2D map $\bs{m}$. Fig.~\ref{fig:imageProcessing}c shows an image taken by the camera on the robot's right side, and hence, the number is included into the map at the same side, as shown in Fig.~\ref{fig:imageProcessing}b. Given that the goal of signing rooms is to provide a unique door sign for each of them, it is assumed that there are not two door signs in a corridor identified by the same number. After receiving $\mathbf{L}$, it must be merged with the numbers of the door signs from the nearest segment of $\mathbf{c}$. For this process, it is important to define $S(\mathbf{c})$ as a function that returns the nearest segment of a cell $\mathbf{c}$, and $L(\cdot)$ as a function that returns the list of door signs from a segment. Thus, each door number $l \in \mathbf{L}$ is included in the list of door signs from the segment of $\mathbf{c}$, $l~\cup~L\big(S(\mathbf{c})\big)$. Besides, each $l$ has an occurrence number, that increases by one every time that the image processing algorithm recognises it. If the robot revisits a place and recognises a $l$ that already exists in $L\big(S(\mathbf{c})\big)$, then its occurrence number is summed to the one in $L\big(S(\mathbf{c})\big)$. 

\begin{figure}[!h]
\centering
\includegraphics[width=.9\textwidth]{figs/ImageProcessing.jpeg}\\ 
(a)\hspace{4.2cm}(b)\hspace{4.2cm}(c)
\caption{Example of the Image Processing module processing two images, in which (a) is an image taken from the left camera, and (c) an image taken from the right camera. (b) shows the 2D map of the environment and the position of the door sign number 228.}
\label{fig:imageProcessing}
\end{figure}

%\section{Semantic Planner}
\label{sec:semanticPlanner} 
The previous Section~\ref{sec:method} explained the necessary components that compose the basis of our semantic AVS system, i.e. the Mapping, Segmentation and Image Processing modules. The explanation continues with the Semantic Planner module, presenting how the planner decides whether the robot should continue its search to find the target, or change its path to a known region. To facilitate the explanation, imagine that the robot has partially visited the environment while running the necessary components of the AVS system. Then, the regions are mapped, segmented, and all the visited doors were recognised. Assuming the existence of such a map, aimed to help the Semantic Planner explanation, this section describes in details the planner.

Our semantic planner is composed of five different parts, in which two of them are semantic-based, Growing Direction and Parity, and the other three are geometric-based, Doors and Robot Orientations, and Distance. The combination of them leads to a planner that is neither exclusively semantic or geometric. This non-exclusivity characteristic is suitable for situations where the environment does not have semantic cues to be considered by our approach. All the five factors are presented individually in this section, introducing the semantic-based firstly, and then the geometric-based ones. However, as this section follows a top-down fashion to introduce the whole planner, the final formula that combines all the five factors is presented before them. Therefore, the reader can have a general idea of how the factors are used and later understand how they work.

\subsection{Final formula}
\label{subsec:finalFormula}
%Given that all the five factors are presented and detailed, now we introduce their combination, and how it decides which candidate cell from $\mathbf{C}$ is the best one to the robot finds the goal-door. 
During the searching process, our semantic AVS system analyses the environment while the robot has not found the goal-door. If it realises the current region of the environment is not promising, the system guides the robot to another direction. To decide the best frontier to go given the set of frontiers, for each candidate cell $\mathbf{c} \in \mathbf{C}$, the planner calculates its attractiveness factor $\varphi(\mathbf{c})$. This factor is the outcome of the combination of five factors briefly presented earlier. These candidate cells in $\mathbf{C}$ are the ones in the centre of the free space, i.e. in the Voronoi, and within a frontier, that is the boundary between visited and not visited cells. Graphically speaking, five candidate cells are shown in Fig.~\ref{fig:2dmapSegmentation_a}, represented by the red dots near the pink line ends. In this case, $\mathbf{C} = \{\mathbf{c}_1, \mathbf{c}_2, \cdots, \mathbf{c}_5\}$. The visited cells are the free cells that were within $\mathbf{T}$, represented by the yellow region in Fig. \ref{fig:2dmapSegmentation_b}, whereas the white region represents the free space that is not close enough to the kernel centred at robot's position. The Final formula explanation is divided into two parts, in which the semantic factor is presented before the geometric factor. In the following subsections, the components of each factor, i.e. Growing Direction, Parity, Robot and Door Orientations, and Distance, are presented.   

The semantic factor, $\mathtt{S}(\mathbf{c})$, combines the Growing Direction and the Parity factors, $\varphi_g(\cdot)$ and $\varphi_p(\cdot)$, respectively. The idea of the first factor is to return high values when the segment $S(\mathbf{c})$ is more likely to contain the goal-door given the door sign sequence. On the other hand, the second part of $\mathtt{S}(\mathbf{c})$ aims to analyse the parity of $S(\mathbf{c})$ and compare it to the goal-door parity. When the robot is in a $S(\mathbf{c})$ that is not likely to contain the goal-door, either due to $\varphi_g(\cdot)$ or $\varphi_p(\cdot)$, it should go to another path and continue the active search. Given that both Growing Direction and Parity factors are important, in $\mathtt{S}(\mathbf{c})$ they are multiplied by each other. If one is low, the result of $\mathtt{S}(\mathbf{c})$ will end up being low as well, even when the other is high. It is important to highlight that $\mathtt{S}(\mathbf{c})$ is completely probabilistic, and due to how both $\varphi_g(\cdot)$ and $\varphi_p(\cdot)$ are modelled, $\mathtt{S}(\mathbf{c})$ becomes robust to outliers. The Semantic factor is given by
\begin{equation}
        \mathtt{S}(\mathbf{c}) = \varphi_g(\mathbf{c}) \varphi_p(\mathbf{c})
\end{equation}

Differently, the geometric factor, $\mathtt{G}(\mathbf{c})$, multiplies the Robot and the Door Orientation factors, $\varphi_r(\cdot)$, $\varphi_o(\cdot)$ respectively, by the Distance one, $\varphi_d(\cdot)$, once the further they are, the less they matter. Then, the outcome of these multiplications is summed to the $\varphi_d(\cdot)$. The geometric factor is given by
\begin{equation}
        \mathtt{G}(\mathbf{c}) = \frac{\big(\varphi_o(\mathbf{c}) + \varphi_r(\mathbf{c})\big)\varphi_d(\mathbf{c}) + \varphi_d(\mathbf{c})}{3.0}
        \label{eq:geometricfactor}
\end{equation}

Finally, in order to define the best $\mathbf{c} \in \mathbf{C}$, i.e. the $\mathbf{c}$ that is more like to contain the goal-door, each of them is submitted to the Eq.~\ref{eq:finalEquation}. Here, $\alpha$ is a threshold that controls the importance of the $\mathtt{S}(\mathbf{c})$ and $\mathtt{G}(\mathbf{c})$, and it ranges as $1 \leq \alpha \leq 0$. The outcome of Eq.~\ref{eq:maxfinalEquation} is the $\mathbf{c}_*$, that is the candidate cell in which its $S(\mathbf{c})$ is more likely to contain the goal-door,
\begin{align}
    \varphi(\mathbf{c}) &= \mathtt{S}(\mathbf{c}) * \alpha + \mathtt{G}(\mathbf{c}) * (1.0 - \alpha) \label{eq:finalEquation}\\
    \mathbf{c}_* &= \underset{\mathbf{c} \in \mathbf{C}}{\argmax}(\varphi(\mathbf{c})) \label{eq:maxfinalEquation}
\end{align}


\subsection{Growing Direction factor}
\label{subsec:growingDirection}
Usually, doors of buildings are signed in sequence and sorted (either increasing or decreasing order). For example, the first number of a corridor is smaller than the last one, or in the other way around. This characteristic can be inferred through the door sign sequence analysis. Imagine, for instance, that a robot is in a corridor where the number of the first door sign is larger than the goal-door one, and this corridor has an increasing door sign sequence. Hence, in terms of the Growing Direction factor, the robot should not consider this path as promising, once it is not very likely that its door sequence contains the goal-door. Therefore, the proposed Growing Direction factor, a type of semantic information inferred from the door sign sequence, is highly useful to our semantic AVS system, given that it indicates the door signs organisation in a segment. 

For each $\mathbf{c} \in \mathbf{C}$, the Growing Direction factor first calculates the angle in which the door sign sequence is increasing, $\theta_i\big(S(\mathbf{c})\big)$. To determine it, all the detected door signs of the segment $S(\mathbf{c})$ are considered, $L\big(S(\mathbf{c})\big)$, as illustrated by Fig. \ref{fig:doorsGroingAngle_a}. Then, for all possible pairs of two different door signs, in which one is larger than other, the vector that connects them is computed. Fig. \ref{fig:doorsGroingAngle_b} demonstrates an example for door sign number 1, and how the vectors are computed in pairs, such as (1,2), (1,3), (1,4), and (1,5). \textcolor{black}{As it shows, the door sign number 1 has four vectors, while the door sign number 4 has only one. Just to illustrate, if we align all these detected door signs, as shown by Fig. \ref{fig:doorsGroingAngle_c}, it would be easier to understand that the sum of vectors from Fig. \ref{fig:doorsGroingAngle_b} and the final $\theta_i\big(S(\mathbf{c})\big)$, indicate that the sequence increases to the right. To make this process even clearer, Fig. \ref{fig:doorsGroingAngle_d} repeats the same procedure to the other door signs remaining, i.e. 2, 3 and 4.}. Here, it is important to mention that the door signs, i.e. the yellow circles, were represented within the white area to help the explanation. In the simulator used in our paper, they appear within the grey area, as illustrated in Fig.~\ref{fig:imageProcessing}b.

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_empty.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_a}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_arrows.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_b}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_door_signs_linegedup.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_c}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_all_arrows.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_d}
     \end{subfigure}
     \caption{\small Demonstration of how the increasing angle $\theta_i\big(S(\mathbf{c})\big)$ is computed in a segment. All the detected door signs within the segment, (a), are considered to calculate the $\theta_i\big(S(\mathbf{c})\big)$. The first step, (b), illustrates the vectors from door sign 1 to the other door signs, and it is easier to understand the effect of this vector calculation aligning all the door signs, (c). The final step of the vector computation, (d), shows all the vectors.}
     \label{fig:doorsGroingAngle}
 \end{figure}

%\begin{figure}[!h]
%\centering
%(a)\hspace{5.5cm}(b)\\
%\includegraphics[width=.48\textwidth]{figs/growing_direction_empty.jpeg}
%\includegraphics[width=.48\textwidth]{figs/growing_direction_arrows.jpeg}
%\\(c)\hspace{5.5cm}(d)\\
%\includegraphics[width=.48\textwidth]{figs/growing_direction_door_signs_linegedup.jpeg}
%\includegraphics[width=.48\textwidth]{figs/growing_direction_all_arrows.jpeg}
%\caption{\textcolor{black}{Demonstration of how the increasing angle $\theta_i\big(S(\mathbf{c})\big)$ is computed in a segment. All the detected door signs within the segment, (a), are considered to calculate the $\theta_i\big(S(\mathbf{c})\big)$. The first step, (b), illustrates the vectors from door sign 1 to the other door signs, and it is easier to understand the effect of this vector calculation aligning all the door signs, (c). The final step of the vector computation, (d), shows all the vectors.}}
%\label{fig:doorsGroingAngle}
%\end{figure}

Fig.~\ref{fig:groingDirection} illustrates a partial map from the simulator used in our paper, and it helps to explain the importance of the Growing Direction. The robot has started at the intersection of three corridors, and it has chosen the number 3, i.e. the one on the right. According to the direction of the robot in this corridor 3, the door sign sequence is considered as increasing. Hence, in this current scenario, if the goal-door were 40, for instance, the Growing Direction factor would consider corridor 3 as promising. In contrast to this, the same corridor 3 would be not promising if the goal-door was 2, given that the sequence only increases meaning that the distance from door sign 2 also increases as the robot continues in that corridor. Besides these two examples, which help to understand how the Growing Direction factor behaves, there is a third case that is important to mention. Imagine that the goal-door is 21, this factor would be high until the door sign 20, but after that, its value would decrease as the robot continue in corridor 3 and the door sign sequence increases. Hence, just by the Growing Direction factor and regardless the parity of both the goal-door 21 and the door signs within the sequence, the robot should continue its search in either the corridor 1 or 2. 

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{figs/all_doorlabelsequences.jpeg}\\
(a)\hspace{4.5cm}(b)\hspace{4.5cm}(c)
\caption{Partial 2D map of the environment, showing three different corridors and many door signs. All images represent the same part of the environment, but (a) shows the simple 2D grid map, (b) shows the visited region, and (c) shows the two segments of the map.}
\label{fig:groingDirection}
\end{figure}

One possible solution to deal with the aforementioned third case is to measure whether all the door signs within the sequence are smaller or larger than the goal-door. Hence, the amount of door signs that are smaller or larger than the goal-door are counted by the functions $L^<\big(S(\mathbf{c})\big)$ and $L^>\big(S(\mathbf{c})\big)$, respectively. The factor $\zeta(\cdot)$ measures the possibility of a segment to have door signs smaller or larger than the goal-door, defined by 
\begin{equation}
\zeta\big(S(\mathbf{c})\big) = \frac{\Big(L^<\big(S(\mathbf{c})\big) - L^>\big(S(\mathbf{c})\big)\Big)}{\max\Big(L^<\big(S(\mathbf{c})\big)+L^>\big(S(\mathbf{c})\big), w_g\Big)},
\label{eq:growingdirectionwg}
\end{equation} 
where $-1 \leq \zeta\big(S(\mathbf{c})\big) \leq 1$, in which $\zeta\big(S(\mathbf{c})\big) = 1$ means that in $S(\mathbf{c})$ there are only larger door signs, $\zeta\big(S(\mathbf{c})\big) = -1$ only smaller door signs, and $\zeta\big(S(\mathbf{c})\big) = 0$ that both  $L^<$ and $L^>$ are equal. $w_g$ is a threshold used to control the minimum amount of detected door signs are necessary to this equation reaches 1 or -1.

In addition, Growing Direction factor also considers $\theta_f(\mathbf{c})$, that is the Voronoi angle at cell $\mathbf{c}$. The difference angle between $\theta_f(\mathbf{c})$ and $\theta_i\big(S(\mathbf{c})\big)$, measured by $\gamma(\theta_f(\mathbf{c}))$, indicates whether $\theta_f(\mathbf{c})$ is pointing to the same direction than $\theta_i\big(S(\mathbf{c})\big)$. Then,\\
\begin{equation}
\gamma(\theta_f(\mathbf{c})) = 1.0 + \left | \frac{\theta_f(\mathbf{c}) - \theta_i\big(S(\mathbf{c})\big)}{\pi} \right | * - 2.0,
\end{equation}
where $-1 \leq \gamma(\theta_f(\mathbf{c})) \leq 1$, in which $\gamma(\theta_f(\mathbf{c})) = 1$ means that $\theta_f(\mathbf{c})$ and $\theta_i\big(S(\mathbf{c})\big)$ are pointing at the same direction, and $\gamma(\theta_f(\mathbf{c})) = -1$ that they are pointing to opposite directions. 

Now, the Growing Direction factor of a cell $\mathbf{c}$, $\varphi_g(\mathbf{c})$, is defined as
\begin{equation}
\varphi_g(\mathbf{c}) = \frac{\zeta\big(S(\mathbf{c})\big) * \gamma\big(\theta_f(\mathbf{c})\big) + 1.0}{2.0},    
\end{equation}
where $-1 \leq \varphi_g(\mathbf{c}) \leq 1$, in which $\varphi_g(\mathbf{c}) = -1$ means that is less likely to reach the goal-door given how the door signs are set in $S(\mathbf{c})$, whereas $\varphi_g(\mathbf{c}) = 1$ means that is high likely. When $\varphi_g(\mathbf{c}) = 0$, it means that the Growing Direction factor is not sure about either the growing direction angle, or about the smaller and larger numbers. Hence, it can not indicate whether $\mathbf{c}$ is a very likely frontier.
 
\subsection{Parity factor}
\label{subsec:parity}
This factor considers the characteristics of a door sign to be either \textit{even} or \textit{odd}, the kind of information that is not explicitly available in the environment, but it can be easily inferred after the number recognition. The idea is to attribute a high probability to corridors that contain mostly door signs with the same parity than the goal-door. It is important to mention that this factor also considers the case in which a corridor contains both even and odd door signs since the probability is proportional to their respective amount.

To calculate the Parity factor, first the amount of door signs from $S(\mathbf{c})$ that has the same or different parity than the goal-door are counted, given by the functions $L^=\big(S(\mathbf{c})\big)$ and $L^{\neq}\big(S(\mathbf{c})\big)$, respectively. Then, for a cell $\mathbf{c} \in \mathbf{C}$, its Parity factor, $\varphi_p(\mathbf{c})$, is given by
\begin{equation}
    \varphi_p(\mathbf{c}) = 0.5 + \frac{L^=\big(S(\mathbf{c})\big) - L^{\neq}\big(S(\mathbf{c})\big)}{\max\Big(L^=\big(S(\mathbf{c})\big) + L^{\neq}\big(S(\mathbf{c})\big), w_p\Big)}*0.5
    \label{eq:paritywp}
\end{equation}
where $0 \leq \varphi_p(\mathbf{c}) \leq 1$, and $w_p$ is a threshold used to control the minimum amount of detected door signs that are necessary to this equation reaches 0 or 1. When $\varphi_p(\mathbf{c}) = 1$, it means that all the observed doors have the same parity than the goal-door, whereas $\varphi_p(\mathbf{c}) = 0$ is the opposite. When $\varphi_p(\mathbf{c}) = 0.5$, it means that $L^{\neq}$ and $L^=$ are equal, and therefore is not possible to ensure the parity of the segment $S(\mathbf{c})$.

\subsection{Robot and Door Orientation factors}
\label{subsec:robotDoorOrientation}
The robot moves through the environment, and it detects door signs as they are in its path. Usually, the position of doors follows a pattern, that includes the possibility of existing doors only on horizontal or vertical corridors, for instance. Therefore, aiming to find the goal-door quickly, it is better to prioritise corridors that are in the same orientation than the already visited ones containing many doors. If the robot can prioritise the corridors in the same orientation, by consequence, its most common orientation will be an angle similar to these corridors.  

The scenario in Fig.~\ref{fig:orientationFactors} illustrates the importance of both Robot and Door Orientation factors. The robot starts at the intersection of corridors 1 and 2 and goes to the right, where it finds a second intersection between corridors 3 and 4, and it has to decide which one it should take. At this moment, all the door signs were detected while the robot had its orientation near \ang{0}. Besides, the robot has most of the time moved heading \ang{0}, as illustrated by the pink line in Fig.~\ref{fig:orientationFactors}a. Hence, when the robot has to decide between corridor 3 or 4, both Robot and Door Orientation factors would guide the robot to corridor 3.  It is due to the orientation of corridor 3 ($\sim$\ang{360}), that has a smaller difference to \ang{0}~than to the orientation of corridor 4 ($\sim$\ang{270}).  

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{figs/all_direction.jpeg}\\
(a)\hspace{4.5cm}(b)\hspace{4.5cm}(c)
\caption{Partial 2D map of the environment, showing four different corridors and many door signs. All images represent the same part of the environment, but (a) shows the simple 2D grid map, (b) shows the visited region, and (c) shows the five segments of the map.}
\label{fig:orientationFactors}
\end{figure}

To calculate the Door Orientation factor, it is considered a history of the $\lambda_d$ most recent robot's orientations when a door sign was detected. Based on this history, it is computed a histogram of such orientations, in which each bin saves the percentage of each possible robot's orientation. Then, given the orientation of $\mathbf{c}$, $\theta_f(\mathbf{c})$, it is consulted in the robot's orientation histogram the probability of finding a door sign considering such orientation, 
\begin{equation}
\label{eq:doorOrientation}
    \varphi_o(\mathbf{c}) = H_d[\theta_f(\mathbf{c})],
\end{equation}
where $H_d[\cdot]$ is the door orientation histogram, and the Door Orientation factor is $0 \leq \varphi_o(\mathbf{c}) \leq 1$, in which 1 is 100\% and 0 is 0\%.

The Door and Robot Orientation factors are very similar to each other. The difference between them is that the first one saves the robot's orientation only when a door sign has been recognised. Therefore, it prioritises the $\theta_f(\mathbf{c})$ that has the highest $H_d[\theta_f(\mathbf{c})]$, i.e. the orientation in which the robot has detected most of the door signs. On the other hand, the idea of the second one, Robot Orientation factor, is to prioritise the $\theta_f(\mathbf{c})$ that is most similar to the robot's orientation that is more frequent, without considering when the door signs were recognised. This factor makes the robot takes into consideration other paths that despite not having door signs, may connect to other ones more promising.

As the robot moves through the environment, its $\lambda_r$ most recent orientations are saved, and they are used to compute a histogram. Each histogram bin represents an angle and the percentage of it in the history of the robot's orientation. Given the calculated histogram, the $\theta_f(\mathbf{c})$ is used as an index to get the probability of that angle, as presented by
\begin{equation}
    \varphi_r(\mathbf{c}) = H_r[\theta_f(\mathbf{c})],
\end{equation}
where $H_r[\cdot]$ is the robot orientation histogram, and the Robot Orientation factor is $0 \leq \varphi_r(\mathbf{c}) \leq 1$, in which $\varphi_r(\mathbf{c}) = 1$ means that $\theta_f(\mathbf{c})$ is an orientation that is equal to the unique robot's orientation saved, whereas $\varphi_r(\mathbf{c}) = 0$ means that $\theta_f(\mathbf{c})$ is an orientation that the robot did not do.

\subsection{Distance factor}
\label{subsubsec:distance}
The fifth factor that is considered by our Semantic Planner is the distance between the robot cell $\mathbf{c}_r$ and each $\mathbf{c} \in \mathbf{C}$, i.e. the smallest number of Voronoi cells that connects each pair of $(\mathbf{c}_r, \mathbf{c})$. Its goal is to guide the robot towards the closest $\mathbf{c}$, instead of spending battery and time going to a farthest one. Take the Fig.~\ref{fig:distanceFactor} as an example, and suppose that the goal-door is 71. The robot has started at the intersection between corridors 1 and 2, and it has moved to the right corridor, Fig.~\ref{fig:distanceFactor}a. After a few minutes, guided by the Robot and Door Orientation factor, it has chosen to continue the searching on corridor 3. Even though this corridor has the same parity than the goal-door (both are odd) the Growing factor indicates that corridor 3 is not promising. Therefore, the robot should continue the searching in one of the other three options, corridors 1, 2 or 4. The Distance factor is responsible for indicating the closest option to the robot, given by the sum of green cells that connect the robot's current position and each red point near the numbers 1, 2 and 4, as shown in Fig.~\ref{fig:distanceFactor}b. 

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{figs/all_distance.jpeg}\\ 
(a)\hspace{4.5cm}(b)\hspace{4.5cm}(c)
\caption{Partial 2D map of the environment, showing four different corridors and many door signs. All images represent the same part of the environment, but (a) shows the simple 2D grid map, (b) shows the visited region, and (c) shows the six segments of the map.}
\label{fig:distanceFactor}
\end{figure}

The first step of the Distance factor calculation is to find the smallest distance $d_{\ll}$ between $\mathbf{c}_r$ and all $\mathbf{c} \in \mathbf{C}$, considering only the Voronoi cells in $\bs{m}$, in which one cell is considered as one to the distance sum. It is given by
\begin{equation}
    \mathbf{c}_{\ll} = \underset{\mathbf{c} \in \mathbf{C}}\argmin\big(D(\mathbf{c}_r, \mathbf{c})\big)
\end{equation}
where $D(\cdot, \cdot)$ is the function that counts the number of cells between two other specific cells. In this factor, only Voronoi cells are counted, regardless they are within mapped or unknown regions.

The idea is that the Distance factor of $\mathbf{c}$, $\varphi_d(\mathbf{c})$, should be high to small distances, and low to the big ones, i.e. give more preference to $\mathbf{c}$ that are closer to the robot. Then
\begin{equation}
    \varphi_d(\mathbf{c}) = 1.0 - \Big(1.0 - \frac{D(\mathbf{c}_r, \mathbf{c}_{\ll})}{D(\mathbf{c}_r, \mathbf{c})}\Big)^4,
\end{equation}
where $0 \leq \varphi_d(\mathbf{c}) \leq 1$, in which $\varphi_d(\mathbf{c}) = 1$ means that $D(\mathbf{c}_r, \mathbf{c})$ is equal to $D(\mathbf{c}_r, \mathbf{c}_{\ll})$, and $\varphi_d(\mathbf{c}) = 0$ means that $D(\mathbf{c}_r, \mathbf{c})$ is so high that makes the division be around zero. 

\section{Experiments and Results} 
\label{sec:experimentsResults}
This section presents the results of simulated and physical experiments. Section \ref{subsec:experimentSetup} explains the software setup used in our simulated experiments, as well as the differences between the physical and simulated experiments. Section \ref{subsec:comparativeMethods} presents the results of the simulation phase, comparing the performance of our proposed semantic AVS system and an entirely geometric AVS system, called \textit{Greedy}. Four different maps were considered in this comparison. Section~\ref{subsec:humanresults} introduces a second type of comparison, in which these two initial AVS systems, ours and the Greedy one, are compared to human participants teleoperating the robot in the simulation setup while performing the searching task. Finally, Section~\ref{subsec:realResults} demonstrates how our semantic AVS system performs in the physical world, as well as the information about where this test was performed. 

\textcolor{black}{It is also important to report the parameters used by our approach throughout all the experiments presented below, either in simulation or in the real world. Both $\textit{w}_g$ and $\textit{w}_p$ are set to eight. This means that in Eq. \ref{eq:growingdirectionwg} and \ref{eq:paritywp}, respectively, the closer or higher to eight the number of detected door signs is, more important the Growing Direction and Parity factors become. The number eight was chosen to balance the importance of the factors since a small number would make them important very soon in the search process, and a large number would play the opposite role. In addition to these two parameters, the Robot and Door orientation factor also have some parameters. The size of the $H_d[\cdot]$ is four, which is the outcome of dividing the range of [\ang{0},\ang{179}] by \ang{45}. It means our approach considers the robot's orientations when detecting a door sign in groups of \ang{45} (e.g. if the robot detects a door sign and its orientation is \ang{42}, $H_d[0]$ is incremented). For the histogram $H_d[\cdot]$, we consider the past $6.000$ orientations, as we read hundreds of robot's orientation per minute, and this reading is noise. For the case of $H_r[\cdot]$, we assume a finer setup, since the robot may be in a different orientation in the range of [\ang{0},\ang{359}]. The size of $H_r[\cdot]$ is 18, and we consider the past $600.000$ readings, due to our high reading rate from the robot's orientation, the presence of noise in the data, and to reduce the impact of an unexpected turning that may happen.}

\subsection{Simulated Experiment Setup}
\label{subsec:experimentSetup}

The setup of the simulated experiments is represented in Fig.~\ref{fig:simulators}. The MobileSim simulates a Pioneer 3-DX robot equipped with a \ang{180}~Laser, providing its odometry information and its laser sensor readings, Fig.~\ref{fig:simulators}a. However, MobileSim does not provide information from door signs, which is vital for the tests in our paper. Therefore, we developed a door simulator (DS) to mimic both the two RGB cameras that are embedded in the physical robot and the Image Processing module that recognises the numbers, Fig.~\ref{fig:simulators}c. DS provides numbers of door signs and their positions in the world when they are within the robot's field of view. Then, the final setup is a combination of the MobileSim to read the robot's information, and our DS that provides the door signs information, as illustrated in Fig.~\ref{fig:simulators}. %As the robot moves through the map in the MobileSim, the DS provides the number and the pose of the door labels if the robot is in front of one.

\begin{figure*}[!h]
\centering
\includegraphics[width=1\textwidth]{figs/drawing2.jpeg}\\
(a) \hspace{4.5cm} (b) \hspace{4.5cm} (c)
\caption{Software setup used in the simulated experiments. It shows the MobileSim in (a), (b) represents the robot's and door signs information as input to our semantic AVS system that returns the robot's next movements, and (c) is the door signs map as ground-truth in the Door simulator. Both (a) and (c) represent the same position on the map.}
\label{fig:simulators}
\end{figure*}

The first evaluation of our semantic AVS system was made through the comparison with the Greedy AVS system in simulated indoor environments. The experiments considered four different scenarios, the Table \ref{tab:scenarios} and the Fig.~\ref{fig:simulationMaps} present the details of them. The four scenarios vary considerably regarding the amount of door signs and how they are set, the size of the buildings, and the corridors orientation. The \textit{Normal} and \textit{Inverse} were made aiming to test the AVS systems in scenarios with many long corridors intersecting each other, where the AVS systems are forced to make decisions very often. Due to the high amount of door signs in both scenarios, four door signs were chosen as goal-doors for the tests, one in each horizontal corridor. Their difference is that \textit{Normal}, Fig.~\ref{fig:simulationMaps}a, has its door signs sequence increasing from the middle to the borders, whereas the \textit{Inverse}, Fig.~\ref{fig:simulationMaps}b, is in the other way around. This way, we can test the performance of our semantic AVS system in different door signs arrangement. The \textit{Hotel} is the third scenario used in the experiments, and it is the third and fourth floors of the Hotel Pennsylvania~\cite{McKim1919Hotel} located in New York. With the highest amount of door signs and a large environment containing many door signs and long corridors, Fig.~\ref{fig:simulationMaps}c, the \textit{Hotel} scenario aims to test the AVS systems in terms of how our semantic AVS system analysis the numbers from door signs. A bad choice in \textit{Hotel} may cause a long run that will not lead to the goal-doors. The fourth scenario is from a public dataset called KTH Campus\footnote{It was used the left building from the floor plan identified as 0510028829\_A30\textendash00\textendash07, A0043015. The dataset can be found at \url{http://www.csc.kth.se/~aydemir/KTH_CampusValhallavagen_Floorplan_Dataset.tar.bz2}}, Fig.~\ref{fig:simulationMaps}d, that contains more than 38,000 rooms in total, considering the many floor plans from different buildings~\cite{Aydemir2012What}. Even though the particular floor plan chosen for this test, called \textit{KTH} scenario, has the lowest amount of door signs compared to the other scenarios used in the tests, it presents corridors in a different orientation than the first three ones. All tests in the simulation were carried out in a laptop 8GB RAM and processor \textit{i7}. 

\begin{table}[h]
\centering
\caption{Different scenarios used on our simulated tests.}
\label{tab:scenarios}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Name}  & \textbf{\# of Door signs} & \textbf{Goal-doors}        \\ \hline
\textit{Normal}           & 113                        & 54, 55, 111, 124  \\ \hline
\textit{Inverse}           & 116                        & 54, 55, 111, 124  \\ \hline
\textit{Hotel}             & 124                        & 76, 135, 148, 185 \\ \hline
\textit{KTH}             & 47                        & 756 \\ \hline
\end{tabular}
\end{table}

\begin{figure*}[p!]
\centering
\includegraphics[width=.4\textwidth]{figs/map_normal_redcircles.jpeg} \\ 
(a) \\
\includegraphics[width=.4\textwidth]{figs/map_inverse_redcircles.jpeg} \\ 
(b) \\
\includegraphics[width=.43\textwidth]{figs/hotel2_PAPER.jpeg} \\ 
(c) \\
\includegraphics[width=.43\textwidth]{figs/KTH_CampusValhallava_finished.jpeg}\\
(d)
\caption{The four maps used in the simulated experiments. The green squares represent the position where the robot has started, and the red circles highlight the goal-doors. The maps are \textit{Normal} (a), \textit{Inverse} (b), \textit{Hotel} (c), and \textit{KTH} (d).} 
\label{fig:simulationMaps}
\end{figure*}

\subsection{Semantic and Greedy AVS systems}
\label{subsec:comparativeMethods}
Our semantic AVS system was early introduced in Sections~\ref{sec:method} and~\ref{sec:semanticPlanner}. In this section, the performance of our system is compared to the Greedy AVS system, which has the exact same basis presented in Section~\ref{sec:method}, but its planner is composed only by the geometric factor from Eq.~\ref{eq:finalEquation}, i.e. the Eq.~\ref{eq:finalEquation} with $\alpha = 0$. Therefore, the planner of the systems is the only difference, that is responsible for the reasoning over their inputs. In summary, the Greedy AVS system searches for goal-doors based on the nearest frontier, whereas our semantic AVS system considers environmental information aiming to make smarter decisions.

Both systems were tested using the same simulation setup. For the four scenarios, the door signs shown in Tab.~\ref{tab:scenarios} were set as goal-doors. They were chosen to cover as many corridors of the scenarios as possible. For each goal-door, both systems repeated their respective tests ten times to have statistically significant results. For every test, it was measured, in meters, the distance travelled by the robot, from its initial position until it finds the goal-door. The distance travelled is the search cost used in our paper, and hence, the shorter the distance, the better is the system performance. Even though we have not measured and presented the search cost in terms of time, it is important to mention that both our Semantic AVS system and the Greedy one moved the robot with the same velocity. Therefore, the system that provides the shortest distance travelled is also the fastest system. Throughout the tests presented in this Section, in Eq.~\ref{eq:uniformKernel} the $a = 1$, and in Eq.~\ref{eq:corridorNotcorridor} the $\delta = 2$. These parameters means that each cell of the 2D grid map $\bs{m}$ had value 1 to compute the kernel area and that the environment had only two types, corridor and not a corridor, as shown in Fig.~\ref{fig:2dmapSegmentation_c}. %\alpha

Tabs.~\ref{fig:resultsNormal},~\ref{fig:resultsInverse},~\ref{fig:resultsHotel}, and~\ref{fig:resultsKTH} present the results of both approaches in each scenario in the simulated tests, \textit{Normal}, \textit{Inverse}, \textit{Hotel}, and \textit{KTH}, respectively. The colourful columns represent the results achieved by tested AVS systems, in which the first is the result from Greedy AVS system, and the other three are from our semantic one. In the greedy column, the value $0,00\%$ means that $\alpha = 0\%$ in Eq.~\ref{eq:finalEquation}, and hence, the planner becomes fully geometric. In the semantic columns, the same value ranges from $80,0\%$ until $100,0\%$, which means that the $\alpha$ ranges from $0.80$ until $1.0$ in Eq.~\ref{eq:finalEquation}. Hence, it changes the importance of the semantic factor in that equation. Our semantic AVS system was also tested with $\alpha$ ranging from $0.5$ up to $0.7$, but the results were not significant, and they are not presented in the tables. The rows of the tables correspond to the goal-doors used as targets, and each goal-door is evaluated in terms of Median, Average, Standard Deviation, Minimum and Maximum distances. It is also important to highlight that within a row, the colour of the table cells ranges from green to red. Green represents the cell with the smallest value within a row, and red represents the largest one.

The results in Tab.~\ref{fig:resultsNormal}, \textit{Normal} scenario, and in Tab.~\ref{fig:resultsInverse}, \textit{Inverse} scenario, are similar in terms of which column has the most red cells. In both cases, our semantic AVS system has a better performance in contrast to the greedy one, since most of the green cells are within the semantic columns, mainly when $\alpha = 80.0\%$ and $\alpha = 90.0\%$. It is also important to highlight that when there are green values within the greedy column, such as the case of door sign $54$ in Tab.~\ref{fig:resultsNormal}, its standard deviation is the highest one for that door sign, $41.59 m$. In Tab.~\ref{fig:resultsInverse}, the lowest average and minimum of the goal-door 111 are from the greedy AVS system, but again its standard deviation is the highest. It means that the ten tests of the greedy system vary considerably, as shown by the difference between its minimum and maximum values. On the other hand, the standard deviation within the semantic columns is lower, meaning that our semantic system has more constant behaviour during the search. It always guides the robot through the same path, making the same decisions in different executions.

\begin{table}[h!]
\centering
  \caption{Results of the greedy and our semantic AVS systems in the \textit{Normal} scenario. All the results are shown in meters.}
\label{fig:resultsNormal}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_NORMAL_NEW.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption{Results of the greedy and our semantic AVS systems in the \textit{Inverse} scenario. All the results are shown in meters.}
\label{fig:resultsInverse}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_INVERSE_NEW.eps}
\end{table}

The Tabs.~\ref{fig:resultsHotel} and~\ref{fig:resultsKTH}, from \textit{Hotel} and \textit{KTH}, present similar results than the two previous tables, in terms of the greedy column having most of the red cells. Besides highlighting that, in general, our semantic AVS system has better performance than the greedy system, both tables also show that our proposed system is efficient in physical scenarios. Even though the $\alpha = 100.00\%$ column within the semantic columns presents satisfying results, mainly in Tab.~\ref{fig:resultsKTH}, a purely semantic AVS system is not always suitable for searching tasks. The geometric factor in Eq.~\ref{eq:finalEquation} is essential and combined with the semantic factor may provide the best results.  

\begin{table}[!h]
\centering
  \caption{Results of the greedy and our semantic AVS systems in the \textit{Hotel} scenario. All the results are shown in meters.}
\label{fig:resultsHotel}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_HOTEL_NEW.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption{Results of the greedy and our semantic AVS systems in the \textit{KTH} scenario. All the results are shown in meters.}
\label{fig:resultsKTH}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_KTH_NEW.eps}
\end{table}

Besides the previous analysis, the optimal solution for each scenario was also measured. It is the shortest path between the starting position, green squares, and a goal-door, red circles, in Fig.~\ref{fig:simulationMaps}. The Tabs.~\ref{fig:shortestNormal}, ~\ref{fig:shortestInverse}, ~\ref{fig:shortestHotel}, and~\ref{fig:shortestKTH} present the optimal solution to each goal-door, as well as the average and standard deviation with each AVS system from Tabs.~\ref{fig:resultsNormal}, ~\ref{fig:resultsInverse}, ~\ref{fig:resultsHotel}, and~\ref{fig:resultsKTH}. 

\begin{table}[!h]
\centering
  \caption{The average and the standard deviations from the \textit{Normal} scenario, Tab.~\ref{fig:resultsNormal}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are shown in meters.}
\label{fig:shortestNormal}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_NORMAL_SHORTEST_NEW.eps}
\end{table}

In general, the difference between the optimal solution of each goal-door and the averages in the same line is larger for results from greedy system than the ones from our semantic AVS system. For example, the optimal solution for the goal-door number 124 in Tab.~\ref{fig:shortestNormal} is $33.16 m$. The average of the Greedy system for the goal-door 124 is $148.5 m \pm 70.08 m$, which is almost five times larger than the shortest path. The results of our semantic AVS system for this same goal-door, in the worst case, is $67.04 m \pm 17.08 m$, which is just two times larger. 

\begin{table}[!h]
\centering
  \caption{The average and the standard deviations from the \textit{Inverse} scenario, Tab.~\ref{fig:resultsInverse}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are in meters.}
\label{fig:shortestInverse}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_INVERSE_SHORTEST_NEW.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption{The average and the standard deviations from the \textit{Hotel} scenario, Tab.~\ref{fig:resultsHotel}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are in meters.}
\label{fig:shortestHotel}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_HOTEL_SHORTEST_NEW.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption{The average and the standard deviations from the \textit{KTH} scenario, Tab.~\ref{fig:resultsKTH}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are in meters.}
\label{fig:shortestKTH}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_KTH_SHORTEST_NEW.eps}
\end{table}

The same analysis can be made for other goal-doors in other scenarios, Tabs.~\ref{fig:shortestInverse} and~\ref{fig:shortestHotel}. Besides this analysis, it is also possible to measure how large the averages are compared to their optimal solution. For all goal-doors and systems tested in the simulation experiments, Tab.~\ref{fig:percentageALL} shows how many times, in percentage, the averages are larger compared to the optimal solutions. For the case of our semantic AVS system from Tabs.~\ref{fig:shortestNormal},~\ref{fig:shortestInverse}, and~\ref{fig:shortestHotel}, it is considered the lowest average between the ones from $\alpha = 80.0\%, \alpha = 90.0\%$, and $\alpha = 100.0\%$.

To compute the percentages presented in Tab.~\ref{fig:percentageALL}, it is considered the optimal solution of each goal-door for each scenario as $100\%$. Hence, if the average is larger than the shortest path, it will be higher than $100\%$,  as the case of the goal-door 54, scenario \textit{Normal}. For the greedy system, it is approximately seven times larger than the optimal solution, i.e. $709.39\%$. 

In Tab.~\ref{fig:percentageALL}, most of the lowest percentages are within the semantic rows. There are few goal-doors in which the greedy system presents the lowest rate. That is the case of goal-door 54 of the \textit{Normal} scenario, and the 111 of the \textit{Inverse}. However, even though the greedy system presents low values, the values from the semantic system to the same goal-doors are close. In contrast to this, analysing the goal-door 54 of the \textit{Inverse} scenario, for instance, the value from the greedy system is almost four times larger. 

\subsection{Human participants performance in object searching task}
\label{subsec:humanresults}
The results presented in Tab.~\ref{fig:percentageALL} illustrate how many times, in percentage, the results of the greedy and our semantic AVS systems are larger than the optimal solution. Some results from the semantic system are two, three or even four times larger, whereas the greedy system provides results that are until 12 times larger than the optimal solution for the scenario \textit{Inverse} and goal-door 54. 

\begin{table}[!h]
\centering
  \caption{Comparison of the optimal solution (shortest path) of each goal-door from each scenario, with the averages from Tabs.~\ref{fig:resultsNormal}, ~\ref{fig:resultsInverse}, and~\ref{fig:resultsHotel}. The shortest path is equivalent to $100\%$, and the figure shows how large the averages are in comparison with the optimal solution.}
\label{fig:percentageALL}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_COMPARISON_PERCENTAGE_NEW.eps}
\end{table}

Given only these high percentages, it seems that both approaches are not suitable for the task of finding a target door sign in an unknown environment based on text information as visual cues. However, it is important to highlight that this task is challenging since the environment is unknown, and there is no way of planning an optimal path a priori. This section illustrates the difficulty level of the searching task by presenting an experiment in which human participants were invited to perform the searching while piloting the robot in the simulator presented in Section~\ref{subsec:experimentSetup}. For this experiment, it was measured (in meters) the distance travelled by the robot, from the initial position until the goal-doors. The distance travelled is the search cost used in our paper, and hence, the shorter the distance, the better is the system performance.

Instead of using the planner of either our semantic or the greedy AVS system to accomplish the finding the goal-door task, ten human participants were invited to teleoperate the robot in the simulation setup, to perform the same role than our semantic planner. The participants were presented to the searching task beforehand, with a time to get familiar with the robot control system and our simulation setup. This experiment aimed to measure human performance in the same setup as the other tested system, to show whether human reasoning provides better results than our semantic AVS system in the same conditions. Therefore, the humans controlled the robot in the same simulator and graphical interface as the two AVS systems, as shown by Fig.~\ref{fig:2dmapSegmentation_a}. The difference of this experiment to the one from Section~\ref{subsec:comparativeMethods} is how the choices are made. In this case, the participants have to choose where the robot must go, playing the planner role to choose the path.

In this experiment, only two scenarios were tested, with one goal-door each. The \textit{Normal} and \textit{Inverse} scenarios are certainly similar, differing only on how the door signs are set. Given that humans are good at memorising what they have seen, it would be unfair to submit them to two similar scenarios, or more than one goal-door for the same scenario. Therefore, the \textit{Normal} and \textit{Hotel} scenarios were chosen. Door signs picked as the target were $111$ to \textit{Normal} and $148$ to \textit{Hotel}, because they are not too far nor too near to the initial position. Hence, the participants would have to explore at least a small part of both scenarios. Throughout all the tests, the only data considered to the evaluation was the travelled distance. 

Tab~\ref{fig:humansresults} summarises the analysis of the ten participants. Besides, it also compares human performance to the greedy and our semantic AVS systems. As in the previous tables, green represents the cells with the smallest value within a row, and red represents the largest one. As can be seen, our semantic AVS system presents a smaller average in both goal-doors, with the lowest indices compared to the others. The minimum travelled distance for the goal-door $148$ is the only case in which our semantic system does not have the lowest result.

\begin{table}[!h]
\centering
  \caption{Human performance to the problem of AVS system. It is compared to the greedy and semantic systems, in which all of them had to find two goal-doors, one in each scenario. The results are presented in meters.}
\label{fig:humansresults}
  \includegraphics[width=.95\textwidth]{figs/RESULTS_HUMANS_NEW.eps}
\end{table}

In contrast to our semantic system, the greedy one presents the worst results for both goal-doors, which confirms the previous results presented in the Section~\ref{subsec:comparativeMethods}. It is also worth to mention the high standard deviation of the Human participants for both goal-doors. It shows that they had different performances, in which some had a shorter travelled distance than others. Some participants did not follow the same pattern when making decisions throughout their run. At the end of their participation, they have reported that they did not have an efficient strategy to search for the target, and no reasoning was made based on the door signs. One specific participant temporarily forgot the goal-door for the \textit{Hotel} scenario, even though it was written in a paper that was in front of the participants. The human eye has a wider field of view than the two cameras used in this experiment setup, which provides advantages to humans when searching for objects in an unknown environment. However, as previously mentioned, the goal of this experiment was to test humans as the decision-maker within the system. That is why they used the same software setup for the experiments as our semantic and greedy systems. 

Besides the lowest average from our semantic AVS system, there are other advantages in comparison to the participants' results. Its small standard deviation means that the same decisions were taken in all the ten test repetitions, which suggests that our system is not random when it is being executed. On the other hand, the same does not apply for the Human results, which means that every participant has their particular reasoning to make a decision. Hence, some participants are more efficient than others in this kind of tasks. Besides the standard deviation, another advantage is the fact that robots are not disturbed by other moving objects or agents in the environment. Therefore, they can focus on the task, and they do not forget the goal-door, what happened to one of the humans during the experiment. 


\subsection{Experiments Using a Physical Robot}
\label{subsec:realResults}
The experiment with a physical robot was performed in one of the buildings of the Federal University of Rio Grande do Sul, Brazil, where the Phi Robotics Research Lab is located. Fig.~\ref{fig:realMap} shows how this building is organised, as well as the rooms and their door signs. The robot used in this experiment is a Pioneer 3DX from MobileRobots, which is equipped with a Lidar laser scan of \ang{180}~and two RGB cameras, as shown in Fig.~\ref{fig:robotSetup}.

\begin{figure}[h]
\centering
  \includegraphics[width=.955\textwidth]{figs/real_environment_map.jpeg}
  \caption{Map used in the experiments with the real robot. It is the building where the Phi Robotics Research Lab is located at the Federal University of Rio Grande do Sul, Brazil. The green square represents the position where the robot starts, and the red circle highlight the goal-door.}
\label{fig:realMap}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=.9\textwidth]{figs/real_scenario_labeled2.jpeg}
  \caption{The Pioneer 3DX robot used in the real environment experiment, as well as the two embedded cameras. The door signs of the environment are also depicted on this figure.}
\label{fig:robotSetup}
\end{figure}

The goal of this experiment is to prove that our semantic AVS system works in physical scenarios, meaning that the robot should be able to find the target goal-door travelling the shortest distance as possible. %Even though the Image Processing module that detects the text in images is not part of the contribution of this paper, we also aim to test its performance as part of our approach. The simulation tests do not use this module to recognise the door labels. Therefore, this is the opportunity to measure its impact on the other parts of our approach.
For this experiment, the goal-door 232 is chosen as the target, which is located on the left side of the initial position (green square), Fig.~\ref{fig:realMap}. Even though it was at the same corridor as the initial position, the experiment setup is good to prove that our system can reason over the detected door signs. From the initial position, the robot can turn to the left or the right. If it decides the left direction, it will find the goal-door quicker, but the other direction would take it to the opposite side. For this case, as soon as a few door signs have been detected, our semantic system would be able to reason over them and infer that this direction is not promising, and hence, it should change to the opposite direction. Therefore, this would show that the door signs are used by our semantic system to find the goal-door efficiently. 

%\begin{figure*}[!h]
%\centering
%  \includegraphics[width=.45\textwidth]{figs/1_Real_Exploration_11.eps}
%  \includegraphics[width=.45\textwidth]{figs/2_Real_Exploration_22.eps}\\
%  \includegraphics[width=.45\textwidth]{figs/3_Real_Exploration_33.eps}
%  \includegraphics[width=.45\textwidth]{figs/4_Real_Exploration_44.eps}\\
%  \includegraphics[width=.45\textwidth]{figs/5_Real_Exploration_55.eps}
%  \includegraphics[width=.45\textwidth]{figs/6_Real_Exploration_66.eps}
%  \caption{Step-by-step of the performance of our semantic AVS system in the physical environment. \textit{a} shows the initial position, where the robot has started the searching, whereas \textit{f} shows the final step when the robot has found the goal-door 232.}
%\label{fig:realResults}
%\end{figure*}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/1_Real_Exploration_11.eps}
         \caption{}
         \label{fig:realResults_a}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/2_Real_Exploration_22.eps}
         \caption{}
         \label{fig:realResults_b}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/3_Real_Exploration_33.eps}
         \caption{}
         \label{fig:realResults_c}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/4_Real_Exploration_44.eps}
         \caption{}
         \label{fig:realResults_d}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/5_Real_Exploration_55.eps}
         \caption{}
         \label{fig:realResults_e}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/6_Real_Exploration_66.eps}
         \caption{}
         \label{fig:realResults_f}
     \end{subfigure}     
     \caption{\small Step-by-step of the performance of our semantic AVS system in the physical environment. \textit{a} shows the initial position, where the robot has started the searching, whereas \textit{f} shows the final step when the robot has found the goal-door 232.}
     \label{fig:realResults}
 \end{figure}

The performance of our proposed semantic AVS system, when submitted to finding the goal-door 232 in a physical environment, was similar to the situation described above. Fig.~\ref{fig:realResults} depicts six steps of the system, and all of them show important moments for the searching. In Fig.~\ref{fig:realResults}a, the robot just had finished one complete rotation to map its surroundings and detected the door sign 240. In Fig.~\ref{fig:realResults}b, the robot has chosen to turn right, where the door sign 242 has been found. This figure demonstrates how our planner decides on changing: \textit{i}) the door signs 240 and 242 were recognised in an increasing sequence, and it means that as the robot goes forward, the distance from the goal-door just increases; \textit{ii}) the robot is between two frontiers, and even though the robot is closer to the one that is in front of it, the other is not that far from it; \textit{iii}) the first two door signs were found in a horizontal corridor, and so far the horizontal corridors are the more likely ones to contain other door signs. Combining all this information, the decision at this moment is that the frontier that the robot is following is less promising than the other one that is behind it. In Fig.~\ref{fig:realResults}c, the robot has changed its orientation to the opposite one. The door sign 238 was recognised, which supports the orientation changing. In Fig.~\ref{fig:realResults}d, as the robot has not recognised any other door sign that contradicts its decision, the exploration continues towards the left direction. In Fig.~\ref{fig:realResults}e, it recognises the door sign 234, which indicates a decreasing sequence towards the goal-door. Finally, in Fig.~\ref{fig:realResults}f, the goal-door 232 is detected, and the robot finishes the searching. Unfortunately, due to the COVID-19 pandemic situation, the university where the robot is located and this experiment was conducted is closed, as the health organisations recommend. Therefore, we were not allowed to perform more experiments like this to other goal-doors to show further the good performance of our semantic AVS system in the physical world. 

\section{Related Work}
\label{sec:relatedwork}
Visual object search is a problem that has been studied for many years in the robotics field. The proposed approaches range from multi-agent collaborating to search for an object~\cite{Ye1996Collaborative}, to a single robot actively performing a semantic-based search~\cite{Zeng2020Semantic}. After many years subtopics of research arose within the object search, such as Indirect and Active Visual searches. Despite this long period in which new approaches have been proposed, there are no detailed surveys in the literature shedding light on this latter subtopic. However, it is possible to find comprehensive surveys on wider topics such as salient objects detection~\cite{Borji2019Salient}, visual attention \cite{Begum2010Visual}, and as pointed out in~\cite{Aydemir2013Active}, active vision~\cite{Chen2011Active,Chung2011Search}. It is important to mention that even though it has not been proposed as a survey, in~\cite{Aydemir2013Active} Aydemir et al. presented a comprehensive review of some of the most important works related to AVS. Hence, we review other works not presented in~\cite{Aydemir2013Active}, that are as important to the development of our paper as the presented ones. This review allows us to show how our work compares to the visual object search body of research. It also shows how our system contributes to the advancement of state of the art.

In a series of papers, Aydemir and colleagues presented spatial representations and a different planner to the AVS problem. In~\cite{Aydemir2011Search}, the authors proposed a spatial relation that describes topological relationships between objects. They used that description to create potential search actions for the AVS problem since they aimed to relax the assumption that objects start within the robot's sensory reach. Their spatial representation was improved in~\cite{Aydemir2011Object}, in which they proposed a combination of a 3D metric map, which supports obstacle avoidance and path planning, and a topological map called place map, which maintains the topology of the environment. The outcome of such combination was a conceptual map, which connects symbols representing instance knowledge about the environment with spatial concepts such as objects, room categories or appearances. The spatial relation introduced in~\cite{Aydemir2011Search} was used later in~\cite{Aydemir2011Plan} when they combined semantic cues to guide the object search process in a larger environment. A switching planner combines a continual classical planner, which decides the overall strategy of the search, and a decision-theoretic planner, which uses a probabilistic sensing model to set the low-level observation actions. In this same paper, they also proposed an exploration strategy which considers the object search task, since their start without an initial map of the environment. The next proposal has argued that there is a strong correlation between local 3D structure and object placement, which is called the 3D context. The authors argued in~\cite{Aydemir2012ExploitingAnd} that local 3D shape around objects is a strong indicator of the placement of these objects. Hence, they used a more general model to learn the relationship between 3D context and objects, in contrast to the correlation between objects and the appearance of the environment. The evaluation of their approach was performed considering a large RGB-D dataset, showing the effect of using 3D context in an object detection task. Besides making an RGB-D dataset publicly available in~\cite{Aydemir2012ExploitingAnd}, in~\cite{Aydemir2012What} Aydemir et al. also published a dataset called KTH. In this case, the dataset is composed of a set of floor plans that encompasses, in total, 37 buildings, 165 floors and 6248 rooms. In addition to KTH, another contribution of their work was two methods for predicting indoor topologies and room categories given a partial map of the environment. The goal was to predict what lies ahead in the topology of the environment through its topology. Finally, in~\cite{Aydemir2013Active} the AVS is performed without any initial map, and hence, besides performing the search, their approach explored the environment as well. This was one of their main contributions, that is the balance between exploration and exploitation, which makes the robot explore more regions of the environment only after carefully searching for the object in the known regions. Their proposed AVS system reasons about whether exploit the known part or explore the unknown part, based on a model that describes the distribution over possible extensions to the current world.

The idea of relying on significant and visible landmarks to narrow down the search was not found only in~\cite{Aydemir2012ExploitingAnd}. Zeng et al. exploited background knowledge about common spatial relationships between landmarks and target objects~\cite{Zeng2020Semantic}. Their proposal, called Semantic Linking Maps (SLiM), maintained the belief over the locations of the target object and the landmark. Simultaneously, it accounted for probabilistic inter-object spatial relations. In contrast to the 3D context-based AVS systems, Rasouli et al. proposed an attention-based AVS system~\cite{Rasouli2020Attention}. They argued that an AVS system must be responsive, directive, spatiotemporal, and efficient, which are the characteristics addressed by their model. It embedded visual attention in an n-step decision-making algorithm formalised as a 1st-order Markov process. The use of visual attention increased the robot's awareness of the environment. Hence, they used all relevant visual information that was available, leveraging the spatial and appearance information about the object. Rasouli and Tsotsos also relied on visual attention methods to reduce computational costs on their robotic visual search~\cite{Rasouli2017Integrating}. They proposed a three-pronged probabilistic search algorithm that incorporated three forms of visual attention, that are viewpoint selection, saliency, and object-based models. On their model, the attention is used to generate maps with highlighted areas in the image which are more likely to contain an object of interest. The experiments showed that the proposed three-tier attention framework decreased the search cost in terms of distance travelled, search time, and the number of actions taken. Saidi et al. explored a different robot than the other works that opted for wheeled robots since their AVS system was proposed based on the specificities of a humanoid robot~\cite{Saidi2007Active}. A visibility map, which constrains the sensor parameter space, was used to avoid unnecessary calls to the rating function, that evaluates the interest of a potential next view through the analysis of the theoretical field of view.

It is worth to mention that our semantic AVS system also considers the exploration of unknown environments as part of the problem. We aim to perform AVS in an entire unknown search space, which requires a way of switching between the exploration of unknown regions and the exploitation in already known regions. Hence, it is important to present some works related to exploration. Here, they are divided into two significant groups regarding their goals. First, strategies that aim to explore the whole environment, usually finishing when the robot has visited the entire free area \cite{Quattrini2016ASemantically, Girdhar2014Curiosity}. Second, strategies called goal-directed that aim to reach a goal, such as searching for an object, a room, or a person. 

The papers reviewed in this section use semantic information to improve their findings. Some of them use a semantic map, whereas others use semantic properties from objects. The system proposed by Aydemir et al. \cite{Aydemir2013Active} focuses on a large-scale environment, where the robot should find objects using mainly visual sensing. They affirm that rather than performing an exhaustive search in the area, their system could find the object guiding the robot towards to areas more likely to contain it. The probability is calculated considering extracted semantic cues from appearance, geometry, the topology of the environment, and general semantic knowledge of the indoor space. They showed that the results improved drastically after including a semantic description in their search system.

Differently, the framework proposed by Veiga et al. \cite{Veiga2016Efficient} searches for objects in domestic environments. It is composed of a system that perceives the query object in RGB-D images through an inference process and sensor information. The outcome of this process, called knowledge, is saved and updated in a semantic map. Experiments in a realistic apartment have shown that their framework worked well in practice, presenting a reliable and efficient search approach. 

Another significant work that searches objects in domestic scenarios is Rogers' et al. \cite{Rogers2013Robot}. In contrast to \cite{Veiga2016Efficient} that proposed a modular system, their approach considered the context of the environment. A graph, connecting places and objects within these places, is used to predict the presence (or absence) of objects based on the room categories. The reasoning made over the graph, combined with a planner, is used to perform an object search task. Experiments showed that the robot was able to find objects in the environment.

Talbot et al. \cite{Talbot2016Find} and Schulz et al. \cite{Schulz2015Robot} proposed navigation approaches that are also Goal-Directed, despite not being exploration ones. The idea of an original and abstract map that links symbolic spatial information with observed symbolic information and actual places in the real world was firstly introduced by \cite{Schulz2015Robot}. This map is used to make inferences about the location of places. Later, Talbot et al. \cite{Talbot2016Find} extended the idea of the abstract maps, proposing a novel method that defines the topological structure and spatial layout information encoded in spatial language phrases. The system has shown to complete the task by travelling slightly further than the optimal path.

Despite the good outcomes from the solutions presented by the papers mentioned above, there is still room for improvements. In~\cite{Aydemir2013Active} Aydemir et al. depended on prior semantic knowledge about indoor spaces obtained from databases. Talbot et al. \cite{Talbot2016Find} and Schulz et al.\cite{Schulz2015Robot} depended on a priori abstract maps. Veiga et al. \cite{Veiga2016Efficient} required beforehand information to learn about objects and the environment. Additionally, it used a 3D recognition based framework from the Point Cloud Library (PCL) for object recognition, which is computationally expensive. Rogers et al. \cite{Rogers2013Robot} also implemented PCL to segment data from RGB-D sensor, continuing to cluster the points, what is a heavy workload for computers. It is also important to highlight that none of them has explored the benefits of textual information available in the environment. 

Our proposed AVS system reads the door sign numbers through an efficient computer vision algorithm and analyses them to decide whether the current path is promising for the robot to find the goal-door. It does not require an environment description nor other instruction in advance, which is suitable for tasks in unknown environments. Additionally, it is not computationally expensive, and a simple computer and two cameras embedded in a robot can execute it. Relying on textual information from the environment, and from that infer semantic information, is another contribution of our work in comparison with the other papers reviewed here. Given that no information or map is necessary as a requirement for our system, it is a good solution for entirely unknown environments. %We understand that some buildings are labelled with a configuration that contains both numbers and letters, which is not supported by our system. However, 

\section{Conclusion} 
\label{sec:conclusion}
We proposed a semantic AVS system that relies on semantic information inferred from text within the environment. The proposed system aims to demonstrate that it is possible to take advantage of different sources of information, such as door signs, traffic signs, or outdoor advertisement. Even though our paper has not tested all these different sources, only door signs, the results show that usage of text inferred from signs for robotic solutions is promising. Besides, our paper also intends to encourage the mobile robotics research community to explore the advantages of semantic information for mobile robot tasks. The main contributions of this paper are: 
\begin{itemize}
    \item a robust semantic planner, based on five different factors, that reason over the door signs to find the goal-door travelling the shortest possible distance;
    \item a semantic AVS system which, by using our semantic planner, can reason over the door signs and estimate when the robot is in a non-promising path without any training step;
    \item an analysis of the usage of text information as input to the semantic planner within the AVS system. In general, the analysis shows that our system has better results than humans participants, both in the same simulation setup.
\end{itemize}

Our semantic planner applied to the AVS system presented an excellent performance, mainly when compared to the results from the greedy system and humans performing the searching. Besides providing the shortest distance travelled, and by consequence, it was also the fastest search system, given that the robot moved with the same velocity in all the experiments. It is important to mention that no experiments were conducted in an environment in which its rooms were randomly signed. This is because we believe that in such kind of environment probably not even humans would be able to rely on the random signs and efficiently search for the target door sign, and our semantic AVS system would rely on an input that is not reliable. \textcolor{black}{On the other hand, despite not being random, the \textit{Hotel} map presents very challenging door sign configurations. Some corridors have a door sign sequence that does not increase nor decrease, and that does not have a predominant parity. These conditions do not reflect a well-structured environment, but our proposal still presented a robust performance, with better results than the other tested approach and humans.}

Our semantic planner does not require that the door signs of the environment are set according to a specific pattern, as confirmed by the wide variety of the four tested scenarios. The scenario \textit{Hotel} demonstrates how different the door signs can be located, and according to the feedback from the participants after their participation, the \textit{Hotel} is indeed a little bit confusing for them. 

The results show that our semantic AVS system presented better or similar results than the ones from human participants. However, it is important to highlight that these results were obtained when humans were piloting the robot in the same simulation setup as the other experiments. The human eyes have a wider field of view than cameras, so in this case, it would be unfair to compare the performance of humans against robots if they had different visual sensors. That is why only human reasoning was considered in this paper.

Finally, as future work, \textcolor{black}{we aim to perform more experiments with the physical robot in real world to measure the performance of our proposal in different scenarios. We also intend to make our code publicly available, as it is implemented based on the Robot Operating System (ROS) and hence, it can be easily reused by the research community. The same applies to the door sign simulator developed by us for testing our proposal. About the type of the door sign, we also aim to explore other standards that also includes letters, not only numbers. This achievement would make our proposal suitable for applications in a wider range of environments.} Besides, the potential of machine learning could be applied to the object searching problem. Instead of modelling the growth factor of a sequence or the odd and even factors, a machine learning-based system could learn how the door signs are set within the environment. In addition to that, investigating the other gains of textual information, such as reading the traffic signs to improve the driving performance of autonomous cars, is another topic that should be investigated. \textcolor{black}{Lastly, our approach depends on the map segmentation, which is done by KDE, to group the detected door signs. In KDE, the kernel size changes the total amount and the area of the segments, besides being one of the parameters of our proposal. Therefore, the dependency of this parameter by our proposal should be investigated to make it more robust and stable.} 