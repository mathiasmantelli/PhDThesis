\chapter[NSOS: using numbers to interpret the organisation of unknown environments]{NSOS: using numbers to interpret the organisation of unknown environments}
\label{chap:3_text_os_system}
%\section{Introduction}
%\label{introduction}
The first of our contribution is called Number-based Semantic OS system (NSOS), and its goal is to guide an SR through an unknown environment until it finds a target door sign. The novelty of our system is that is relies on detecting numbers from door signs to estimate the organisation of indoor environments, aiming to reduce the search cost. Besides, another aspect of NSOS is the combination of semantic and geometric clues to improve the search estimations. The former clue type is inferred from the numbers of several door signs found in the environment, and they are useful for understanding how the doors are organised. The latter type is computed from the geometry of the environment, e.g., the corridors and intersections, and it indicates which regions are more likely to finding doors. An example of using our NSOS could be an autonomous courier SR that delivers packages to an older person in a specific appartment within a large-scale and unknown building, where a previous map is not available.

%///////////////////////////////////////////////
This section details the basic modules that compose our NSOS system. It starts with the proposal and contributions in Section~\ref{sec:chap3_proposal_contribution}, followed by the explanation of the basis for our NSOS system in Section~\ref{sec:method}. It details the Mapping module in Section~\ref{subsec:mapping}, to explain how the 2D grid map is built. The Map Segmentation module is introduced in Section~\ref{subsec:mapsegmentation}, which presents how the map is split into different segments. Section~\ref{subsec:imageprocessing} describes how the numbers are extracted from the door signs through computer vision algorithms. The planner of our NSOS system is described in Section~\ref{sec:semanticPlanner}. Finally, this chapter is finished with the experiments in Section~\ref{sec:experimentsResults}, and the summary in Section~\ref{sec:conclusion}.
%///////////////////////////////////////////////


%Humans perform several activities that are part of their routine in everyday living spaces, in which searching for objects or regions of interest (ROI) is one of them. It demands visual recognition and high-level reason as essential skills.~\cite{Dicarlo2012How}. In such a real situation, it is not prudent to assume that the desired object or ROI is always present in the human's field of view since the beginning. The environment navigation is another example of daily activities for humans, and it must be as intelligent as possible, even in unknown places~\cite{Talbot2016Find}. In such cases, humans must actively navigate and search for target objects in the environment, relying on their visual recognition abilities~\cite{Dicarlo2012How,Sjoo2012Topological}. 

%Autonomous robots, which perform tasks related to object searching, such as home assistance, delivery of packages, manipulation in factories, and fetch and carry, also need to fulfill the same visual recognition and navigation requirements as humans. The progress concerning the tasks performed by autonomous robots is possible thanks to the late advances in the field of mobile robotics, more specifically in the localization, mapping, navigation, perception, and exploration ones~\cite{Aydemir2013Active}.
%copiei daqui
%Similar to humans in the context of object searching tasks, robots should also not rely on the assumption that objects are already within their field of view. Hence, they have to find objects in large-scale environments based on primarily visual sensors, which is known as visual object search (OS) problem~\cite{Sjoo2012Topological,Aydemir2013Active}. Briefly, the OS problem computes how to bring the target object into the mobile robot's sensor field of view. 
%ate aqui

%In the context of OS problems, the search strategy is one of the critical factors since it directly impacts the efficiency of a solution~\cite{Aydemir2013Active}. It is responsible for maximizing the probability of detecting the target object in the environment and minimizing the total cost of the task~\cite{Sjoo2012Topological}. There are different approaches to measure this cost, and the most common options are the time or distance traveled, and the robot's movements (each type of movement has a different value). For example, imagine that a courier robot delivers a package to a specific room within a large-scale and unknown building. The most straightforward search strategy would be the robot visiting all places in the environment and visually checking whether every new room is the target one. Even though many robot sensors have a limited field of view, it is very inefficient and time-consuming to make the robot visit all the existing rooms of a building to accomplish its task. In contrast to this simple strategy, semantic information about the environment could be gathered and used by a more efficient planner. Therefore, instead of making the robot exhaustively visit all the existing rooms to deliver the package to the target room, the search strategy would reason over the semantic information and infer important cues to improve the searching.

%copiei daqui
%The research community has proposed essential and valuable works related to the OS problem~\cite{Sjoo2012Topological,Aydemir2013Active,Ekvall2007Object,Sjoo2009Object,Rasouli2020Attention}. However, despite these contributions, the problem is proven to be NP-Complete~\cite{Tsotsos1992Onthe,Ye2001AComplexity}. Then, the optimal search solution can be computed by approximation~\cite{Sjoo2012Topological}, minimizing the search cost as much as possible. In the example of the courier robot previously introduced, taking advantage of semantic information of the environment provides search cues for this approximation. Then, the robot should be able to reason over their sensor readings, infer supplementary knowledge, and increase their level of abstraction of the environment over time \cite{Barber2018Mobile}.
%ate daqui

%However, making the robot get such information from real scenarios, i.e. unexplored environments, includes additional cost and increases the difficulty level of OS systems~\cite{Aydemir2013Active}. The challenge of such systems relies on balancing exploration and knowledge exploitation, i.e. should the robot explore further or search for the target in the already known regions. This chapter considers the scenario where a courier robot delivers a package to a target room in a large-scale and unknown environment, with the shortest possible distance traveled. The searching strategy relies on numbers visually extracted from door signs in the environment to accomplish the task, as any information is provided to the robot beforehand.

\section{Proposal and contributions}
\label{sec:chap3_proposal_contribution}
Our NSOS efficiently searches for a target object that is represented by the number of door signs, called \textit{goal-door}, in large-scale and unknown buildings. 
%We evaluate it in four different simulated indoor environments and one physical test as proof-of-concept implementation in which a robot is tasked with finding a target door sign. Besides, we compare the performance of our NSOS with a purely geometric OS system called Greedy and humans teleoperating the robot to perform the same search task.
Different OS approaches have been proposed by the research community as earlier presented in Chapter~\ref{chap:3_related_work}. In many of them, the robot is tasked with finding objects based on their visual appearance or position in the environment, e.g. an yellow mug on the kitchen table. However, to the best of our knowledge, OS systems that search for door signs (not areas in general) and take numbers (in text format) as input to their strategy are not well explored yet. 

The novelties presented in this chapter in comparison to the already published systems are threefold: 
\begin{itemize}
	\item \textbf{Number from door sign as target object:} The target of our proposed semantic OS system, NSOS, is a specific door sign within an unknown indoor environment. Several other OS systems look for objects that are part of our daily life, such as kitchen utensils or office supplies. However, in some cases finding a door sign is as important as finding the objects from the previous example. The importance is because the door sign is usually associated to a room of the environment, where the SR could perform other tasks afterwards. This novelty enables, for example, autonomous courier SR to perform the final part of the delivery task, which happens after it arrives at the buildings, where there is no map available. Our system is relevant in moments such as the COVID-19 pandemic or any other outbreak. In these situations, people are recommended to avoid social contact, which forces them to stay at home. Hence, the use of fully autonomous SRs for performing the entire delivery task, from the restaurant straight to the costumer's door, saves people of being exposed to the threat, mainly the elderly people. The Starship Technologies company has already deployed their fleet of courier SRs, supporting the importance of robots in food and package deliveries. Another example of how our NSOS could be used will be presented in Chapter~\ref{chap:5_thesis_application}.
	\item \textbf{Organisational semantic information inferred from numbers:} Our NSOS system relies on information inferred from numbers as a visual cue, and more specific numbers extracted from the door signs. Large buildings, for instance, are divided into many small rooms, and usually comply with a pattern of signing each room~\cite{Aydemir2012What,University2012Room,Brian2014Abasic, Stanford2017Room}. Different from depending on color, visual features, or 3D templates, to perform the search, using numbers allows our NSOS to infer high-level information about the environment. Such information may suggest how the environment and the door signs are organised, which is useful during the search. If we could analyze humans behavior while looking for a door sign in an unknown environment, most of them would try to figure out the door signing arrangement. They would avoid exploring the entire building by analyzing how the door signs are related to each other to infer whether the current corridor is promising in terms of containing the goal-door. A courier SR could behave in a similar way to efficiently perform the searching task. Hence, our NSOS system processes the numbers to infer semantic information from them. The inferred information is the odd and even characteristics and whether the sequence of numbers is increasing or decreasing. In addition to the semantic information, our NSOS system also considers geometric information, which is the distance between the estimated robot's pose and the unknown regions, and the history of the robot's orientation.
	\item \textbf{Organisational semantic information to complement the robot's geometric perception:} we also present an analysis of the advantage of using numbers from the door signs and the inferred semantic information as input to our NSOS system. This way, it is not limited to only geometric information. The combination of this semantic information, inferred from the numbers, and the geometric information, extracted from the environment, are useful inputs for the reasoning of our system. It permits the efficient computation of search steps, guiding the robot towards areas more likely to lead to the target room. Lastly, in addition to this computation, which is fully probabilistic, our system also builds a 2D map of the environment. It is segmented based on KDE, introduced in Section~\ref{chap:2_kde_grid_maps}, according to different densities of free space~\cite{Maffei2015Using}. Our system considers the laser readings to build the 2D map, and the images of an RGB camera to extract the numbers from the door signs, and the other types of character are not used for now. The numbers are used to infer semantic information and search cues, whereas the NSOS system indicates which direction is more likely to contain the goal-door to guide the SR.
\end{itemize}

%The remainder of this chapter is organized as follows. After reviewing the literature in Section \ref{sec:relatedwork}, Section \ref{sec:method} describes our semantic OS system and its basic components. Section \ref{sec:semanticPlanner} explains our semantic planner and how it considers the door signs as exploration cues to reach the goal-door. Next, Section \ref{sec:experimentsResults} introduces the experimental setup, and then it compares the results of our semantic OS system to, first, a purely geometric and coverage-based OS system called Greedy, and second to human participants performing the same task using the robot embodiment (teleoperating the robot and observing its sensor readings in the simulation setup). Lastly, it presents the results of our semantic OS system in a proof-of-concept that uses a physical robot performing the search in an unknown environment. The chapter conclusion is presented in Section \ref{sec:conclusion}, discussing the demonstrated outcomes.


%\section{Overview}
%\label{subsec:overview}
%Our proposed system performs OS, i.e., it guides the robot through an unknown environment until the robot reaches a specific location that contains the target object. Our system focuses on indoor environments, such as buildings with many rooms identified by door signs. One of its advantages is that it does not require any a-priory knowledge about the environment, such as the door signs arrangement or where the robot should be heading. The target object to be found is a door sign, which is identified by a number. 
%%%An example to illustrate the usage of our system is a courier robot that delivers a package. From the restaurant until the destination building, it uses Google Maps and its embedded GPS to navigate through the city. However, once inside the building, it does not have a map to plan its path through the corridors. Therefore, the robot has to search the target goal-door to deliver the package to the customer. 

%Our system comprises four modules: Mapping, Image Processing, Map Segmentation, and Semantic Planner. The Semantic Planner module, presented in Section \ref{sec:semanticPlanner}, is the main contribution of this chapter. It requires a base system to work, composed by the first three other modules, that are discussed in Sections \ref{subsec:mapping}, \ref{subsec:mapsegmentation}, \ref{subsec:imageprocessing}. The first of these three modules, Mapping, aims to build a 2D grid map of the environment. For our NSOS system, a 3D map would not be significantly beneficial, and the extra computational cost to build and update a 3D map does not worth it. The next module, Image Processing, processes the images taken by two RGB cameras, and it analyses them to recognize the number from door signs. Once identified, the module updates the 2D grid map to include the numbers at their respective side. For instance, if the robot's left camera captures an image that contains a door sign, such number is drawn in the map, in the left side of the current robot's position. The same applies for the robot's right camera. Figure~\ref{fig:2dmapSegmentation_a} illustrates this process. The third module, Map Segmentation, is responsible for segmenting the free space of the 2D grid map according to its size using the Kernel Density Estimation (KDE) approach introduced by Maffei~\textit{et al.} \cite{Maffei2015Using}. This module also assigns to each segment of a corridor its respective list of door signs, which is the list of numbers recognized while the robot was within the corridor. The last module, Semantic Planner, calculates which path is more likely to contain the goal-door given its detected doors. The Boundary Value Problem (BVP) \cite{Prestes2002Exploration}, calculated over the grid map and the Voronoi diagram \cite{Guo1989Parallel}, moves the robot towards the path that is most attractive, defined by the Semantic Planner.

\section{Basis for NSOS system}
\label{sec:method}

Our proposed system performs OS, i.e., it guides the SR through an unknown environment until the robot reaches a specific location that contains the target object. Our system focuses on indoor environments, such as buildings with many rooms identified by door signs. One of its advantages is that it does not require any a-priory knowledge about the environment, such as the door signs arrangement or where the SR should be heading. The target object to be found is a door sign, which is identified by a number. %An example to illustrate the usage of our system is a courier robot that delivers a package. From the restaurant until the destination building, it uses Google Maps and its embedded GPS to navigate through the city. However, once inside the building, it does not have a map to plan its path through the corridors. Therefore, the robot has to search the target goal-door to deliver the package to the customer. 

Our system comprises four modules: Mapping, Image Processing, Map Segmentation, and Semantic Planner. The Semantic Planner module, presented in Section \ref{sec:semanticPlanner}, is the main contribution of this chapter. It requires a base system to work, composed by the first three other modules, that are discussed in Sections \ref{subsec:mapping}, \ref{subsec:mapsegmentation}, \ref{subsec:imageprocessing}. The first of these three modules, Mapping, aims to build a 2D grid map of the environment. For our NSOS system, a 3D map would not be significantly beneficial, and the extra computational cost to build and update a 3D map does not worth it. The next module, Image Processing, processes the images taken by two RGB cameras, and it analyses them to recognize the number from door signs. Once identified, the module updates the 2D grid map to include the numbers at their respective side. For instance, if the robot's left camera captures an image that contains a door sign, such number is drawn in the map, in the left side of the current robot's position. The same applies for the robot's right camera. Figure~\ref{fig:2dmapSegmentation_a} illustrates this process. The third module, Map Segmentation, is responsible for segmenting the free space of the 2D grid map according to its size using the Kernel Density Estimation (KDE) approach introduced by Maffei~\textit{et al.} \cite{Maffei2015Using}. This module also assigns to each segment of a corridor its respective list of door signs, which is the list of numbers recognized while the SR was within the corridor. %The last module, Semantic Planner, calculates which path is more likely to contain the goal-door given its detected doors. The Boundary Value Problem (BVP) \cite{Prestes2002Exploration}, calculated over the grid map and the Voronoi diagram \cite{Guo1989Parallel}, moves the robot towards the path that is most attractive, defined by the Semantic Planner.

\subsection{Mapping module}
\label{subsec:mapping} %subsec:systembasis
The Mapping module considers that an SR is equipped with a 2D lidar sensor. The sensor measurements are used to build a 2D occupancy grid map $\bs{m}$ of the environment, like shown in Figure \ref{fig:2dmapSegmentation_a}. In simulation, the grid map $\bs{m}$ is built using the Histogramic In-Motion Mapping (HIMM) technique~\cite{Borenstein1991Histogramic}, that takes as input the lidar sensor readings. We assume there is no error in the robot motion, as we explained in Section~\ref{sec:chap2_basic_mobile_robotics}, but this assumption holds true only for the simulation experiments. When carrying out the experiments with the physical robot, we used the SLAM system called GMapping~\cite{Grisetti2007Gmapping}.

Over $\bs{m}$, the Mapping module also computes the Voronoi diagram to have the center cells of the free spaces, represented by the green lines in Figure \ref{fig:2dmapSegmentation_b}. The yellow region represents the free space visited by a circular kernel centered at the robot's position, i.e., the circle centered at the robot's pose. Based on that, the BVP smoothly moves the SR through the environment, avoiding obstacles and keeping it as close as possible to the Voronoi cells.

%\begin{figure}[h]
%\centering
%(a) \hspace{5cm} (b)\\
%\includegraphics[width=0.46\textwidth]{figs/1.jpeg}
%\includegraphics[width=0.46\textwidth]{figs/2.jpeg}\\
%\vspace{.2cm}
%(c) \hspace{5cm} (d)\\
%\includegraphics[width=0.46\textwidth]{figs/3.jpeg}
%\includegraphics[width=0.46\textwidth]{figs/4.jpeg}
%\caption{Example of our mapping and segmentation modules. (a) shows the white area representing the free cells and the pink line the robot path, (b) the marked yellow areas representing the visited cells considered by the kernel of Equation \ref{eq:kerneldefinition}, and the Voronoi through the green lines, (c) our KDE-based module segmenting the visited cells of (b) as two types, and (d) the segment identification using different colours.}
%\label{fig:2dmapSegmentation}
%\end{figure}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/11.png}
         \caption{}
         \label{fig:2dmapSegmentation_a}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/22.png}
         \caption{}
         \label{fig:2dmapSegmentation_b}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/33.png}
         \caption{}
         \label{fig:2dmapSegmentation_c}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/44.png}
         \caption{}
         \label{fig:2dmapSegmentation_d}
     \end{subfigure}
     \caption[Example of our mapping and segmentation modules.]{\small Example of our mapping and segmentation modules. (a) It shows the robot, the white area representing free cells, the pink line as the robot path, and the five frontier points, (b) the marked yellow areas representing the visited cells and the Voronoi through the green lines, (c) the segmented map, with the visited cells from (b) segmented into two different segment types, and (d) the segment identification using different colors with four distinct segments indicated.}
     \label{fig:2dmapSegmentation}
 \end{figure}


\subsection{Map Segmentation module}
\label{subsec:mapsegmentation}
The Segmentation and Mapping modules are executed simultaneously, aiming to split the free space of $\bs{m}$ into multiple regions according to the density of free space. Every segmented region, called a segment $\mathbf{s}$, is associated to a group of cells $\bs{m}_i$ of the grid map $\bs{m}$. In $\bs{m}$, there might be different types of segments. Figure \ref{fig:2dmapSegmentation_c} illustrates the case in which the Segmentation module considers only two types. In this case, all green segments, or all the red ones, have a similar density of free space computed using the KDE. Besides their type, each segment is also singularly identified, such as $\mathbf{s}_j$ for the $j$-th segment in $\bs{m}$. Figure~\ref{fig:2dmapSegmentation_d} shows the segmented 2D map as if each segment was identified as one color, to highlight that they are identified diferently (even though there are only two segment types in such figure).  

%For this purpose, the Segmentation module uses the KDE estimation approach~\cite{Maffei2015Using}. 
KDE computes the estimation of a given region surrounding a cell $\bs{m}_0$, $\Psi(\bs{m}_0)$, which is explained in Section~\ref{chap:2_kde_grid_maps}, centered on $\bs{m}_0$ by
\begin{equation}
\label{eq:kerneldefinition}
    \Psi(\bs{m}_0) = \frac{1}{|T|}\sum_{\bs{m}_i}^\mathbf{T} Q(\bs{m}_i, \bs{m}_0)UK(\left \| \bs{m}_i - \bs{m}_0 \right \|).
\end{equation}
where $K(\cdot)$ is a uniform circular kernel function that over all cells at distance $d \leq r$ from $\bs{m_0}$, defined as
\begin{equation}
    UK(d) = \begin{cases}
1 & \text{, if $d \leq r$} \\ 
0 & \text{, otherwise,} 
\end{cases}
\label{eq:uniformKernel}
\end{equation}
where $r$ is the radius and $d$ is the Manhattan distance from the current cell being measured, $\bs{m}_i \in \mathbf{T}$, to the centre of the kernel, cell $\bs{m}_0 \in \bs{m}$. $\mathbf{T} \in \bs{m}$ is a subset of cells that are at maximum $r$ farther from $\bs{m}_0$. The function $Q(\bs{m}_i,\bs{m}_0)$ tests whether a cell $\bs{m}_i$ is free, and it is defined as
\begin{equation}
\label{eq:freeCell}
    Q(\bs{m}_i, \bs{m}_0) = 
\begin{cases}
1 & \text{, if $\bs{m}_i$ is a cell that belongs to the free space region connected to $\bs{m}_0$} \\ 
0 & \text{, otherwise.} 
\end{cases}
\end{equation}

%Combining the previous function into the KDE approach, it is possible to calculate the kernel density. For a cell $\bs{m}_k$, its free space $\Psi(\cdot)$ is computed by

%The algorithm, in practice, is working over $\mathbf{M}$. Therefore, $\Psi$ is defined as a sum instead of an integral. 

According to Equation \ref{eq:freeCell}, $\Psi(\cdot)$ returns different density estimations based on the number of obstacle or unknown cells are surrounding $\bs{m}_0$. Assuming that the Segmentation module considers different values, and given that Equation~\ref{eq:kerneldefinition} estimates the density of free spaceÂ´ surrounding a cell $\bs{m}_0 \in \bs{m}$, $\Psi(\cdot)$ can be used in the Segmentation module as
\begin{equation}
\label{eq:corridorNotcorridor}
    \Upsilon(\bs{m}_0) = \left \lfloor  \frac{\Psi(\bs{m}_0)}{\delta}  \right \rfloor
\end{equation}
where $\delta$ is a threshold that defines how many different densities of free regions are considered by the segmentation function, $\Upsilon(\cdot)$. %For instance, if $0 \leq \Psi(\mathbf{c}_k) \leq 800$ and $\delta = 400$, Equation \ref{eq:corridorNotcorridor} segments free areas in only two different sizes, as illustrated by Figure \ref{fig:2dmapSegmentation}-(b). On the other hand, if $\delta = 200$, it segments in four sizes. 
Therefore, a high $\delta$ means Equation \ref{eq:corridorNotcorridor} considers few different densities, whereas a low $\delta$ is the opposite. 

A segment $\mathbf{s}$ represents a group of free and adjacent cells from $\bs{m}$ that have the same $\Upsilon(\cdot)$. Figure \ref{fig:2dmapSegmentation_d} demonstrates different segments, in which each one has a different color. For example, given $\bs{m}_0$ and $\bs{m}_1$ as two free and neighbouring cells in $\bs{m}$, and that $\Upsilon(\bs{m}_0) = \Upsilon(\bs{m}_1)$, then both belong to the same segment $\mathbf{s}_0$. Otherwise, a new segment $\mathbf{s}_1$ is created and $\bs{m}_1$ is associated to it. Thus, the segmentation of free adjacent cells from $\bs{m}$ is based on Equation \ref{eq:corridorNotcorridor}.

\subsection{Image Processing module}
\label{subsec:imageprocessing}
The last module that completes the basis of our NSOS system is the Image Processing. It aims to recognize the number of door signs that may be in an RGB image. The idea here is to use one existing character recognition algorithm \cite{Zhang2013Text,Jung2004Text,Neumann2012Realtime}, since this is not the focus of this thesis and any approach can be used. The chosen work is the one proposed by \citet{Neumann2012Realtime} due to its real-time recognition aspect and its robustness against noise, and low contrast of characters. Besides, it does not require any information or preparation beforehand. The work proposed by~\citet{Neumann2012Realtime} is presented in details in Section~\ref{subsec:chap2_ocr}.

For a given image $\mathbf{I}$ that was captured by the robot at cell $\bs{m}_r$, e.g., Figures~\ref{fig:imageProcessing}a and~\ref{fig:imageProcessing}c, this module returns a list, $\mathbf{L}$, containing the recognized number from door signs. In the case of Figure~\ref{fig:imageProcessing}, $\mathbf{L}$ would contain only the number 228. Figure~\ref{fig:imageProcessing} also shows where the detected door signs are included into the 2D map $\bs{m}$. Figure~\ref{fig:imageProcessing}c shows an image taken by the robot's right camera, and hence, the number is included in $\bs{m}$, at the right side of the robot's position, $\bs{m}_r$, as shown in Figure~\ref{fig:imageProcessing}b. Given that the goal of signing rooms is to provide a unique door sign for each of them, it is assumed that there are not two door signs in an environment identified by the same number. In regions where there are multiple door signs in sequence with the same number, our system will consider only one instance of all these door signs. After receiving $\mathbf{L}$, it must be merged with the numbers of the door signs from the segment that $\bs{m}_r$ is associated. For this process, it is important to define $S(\bs{m}_i)$ as a function that returns the segment in which the cell $\bs{m}_i$ is associated with. In addition, there is also the function $L(\cdot)$ that returns the list of door signs from a segment. Thus, each door number $l \in \mathbf{L}$ is included in the list of door signs from the segment of $\bs{m}_r$, $l~\cup~L\big(S(\bs{m}_r)\big)$. Besides, each $l$ has an occurrence number that increases by one every time the image processing algorithm recognizes it. If the robot revisits a place and recognises a $l$ that already exists in $L\big(S(\bs{m}_r)\big)$, then its occurrence number is summed to the one in $L\big(S(\bs{m}_r)\big)$. 

\begin{figure}[!h]
\centering
\includegraphics[width=.9\textwidth]{figs/ImageProcessing.jpeg}\\ 
(a)\hspace{4.2cm}(b)\hspace{4.2cm}(c)
\caption[Example of the Image Processing module processing two images]{Example of the Image Processing module processing two images, in which (a) is an image taken from the left camera, and (c) an image taken from the right camera. (b) shows the 2D map of the environment and the position of the door sign number 228.}
\label{fig:imageProcessing}
\end{figure}

\section{Semantic Planner of our NSOS system}
\label{sec:semanticPlanner} 
The previous Section~\ref{sec:method} explained the necessary components that compose the basis of our NSOS system, i.e., the Mapping, Segmentation, and Image Processing modules. The explanation continues with the Semantic Planner module, presenting how the planner decides whether the SR should continue its search in the current region to find the target, or change its path to a known region. To the rest of this section, imagine that an SR has partially visited the environment while running the necessary components of our NSOS system. Then, there will be a partial map, segmented, and with all the so far recognised doors included. This section presents the details of our planner assuming the existence of such a map, aimed to help the explanation.

Our semantic planner is composed of five different parts, in which two of them are semantic-based, Growing Direction and Parity, and the other three are geometric-based, Doors and Robot Orientations, and Closeness. Combining them them leads to a planner that is neither exclusively semantic nor geometric. This non-exclusivity characteristic is suitable for situations where the environment does not have semantic cues to be considered by our system. All the five factors are presented individually in this section, introducing the semantic-based first and then the geometric-based ones. However, as this section follows a top-down fashion to introduce the whole planner, the computation of the estimation about where the SR should go, which combines all the five factors, is presented before them. Therefore, the reader can have a general idea of how the factors are used and later understand how they work.

\subsection{Combining the geometric and semantic factors to estimate where to go}
\label{subsec:finalFormula}
%Given that all the five factors are presented and detailed, now we introduce their combination, and how it decides which candidate cell from $\mathbf{C}$ is the best one to the robot finds the goal-door. 
Our NSOS system analyses the environment during the searching process while the SR has not found the goal-door. If it realizes the current region of the environment is not promising, the system guides the SR to another direction. To decide the best frontier to go given the set of frontiers, for each candidate cell $\bs{m}_i \in \mathbf{C}$, the planner calculates its attractiveness factor $\varphi(\bs{m}_i)$. This factor is the outcome of the combination of five factors briefly presented earlier. These candidate cells in $\mathbf{C}$ are the ones in the center of the free space, i.e., in the Voronoi, and within a frontier, which is the boundary between visited and not visited cells. Graphically speaking, five candidate cells are shown in Figure~\ref{fig:2dmapSegmentation_a}, represented by the red dots near the pink line ends. In this case, $\mathbf{C} = \{\bs{m}_1, \bs{m}_2, \cdots, \bs{m}_5\}$. The visited cells are the free cells that were within $\mathbf{T}$, represented by the yellow region in Figure \ref{fig:2dmapSegmentation_b}, whereas the white region represents the free cells that is not close enough to the kernel centered at the robot's position. The explanation here is divided into two parts, in which the semantic factor is presented before the geometric factor. In the following subsections, the components of each factor, i.e., Growing Direction, Parity, Robot and Door Orientations, and Closeness, are presented.   

The semantic factor, $\mathtt{SF}(\bs{m}_i)$, combines the Growing Direction and the Parity factors, $\varphi_g(\cdot)$ and $\varphi_p(\cdot)$, respectively. The idea of the first factor is to return high values when the segment $S(\bs{m}_i)$ is more likely to contain the goal-door given the door sign sequence. On the other hand, the second part of $\mathtt{SF}(\bs{m}_i)$ aims to analyze the parity of $S(\bs{m}_i)$ and compare it to the goal-door parity. When an SR is in a $S(\bs{m}_i)$ that is not likely to contain the goal-door, either due to $\varphi_g(\cdot)$ or $\varphi_p(\cdot)$, it should go to another path and continue the active search. Both Growing Direction and Parity factors are essential for our NSOS. If the robot is moving through a corridor where the numbers from the door signs have their parity different from the goal-door, there is no reason to continue the search there. Similarly, if the numbers of a sequence of door signs is increasing (or decreasing) away from the goal door, searching in that region is pointless. Hence, if either of these factors indicate the corridor is not promising, the robot should continue the search in another region, regardless the other factor. Then, in $\mathtt{SF}(\bs{m}_i)$ they are multiplied by each other. If one is low, the result of $\mathtt{SF}(\bs{m}_i)$ will end up being low as well, even when the other is high. It is important to highlight that $\mathtt{SF}(\bs{m}_i)$ is completely probabilistic, and considering to how both $\varphi_g(\cdot)$ and $\varphi_p(\cdot)$ are modelled, $\mathtt{SF}(\bs{m}_i)$ becomes robust to outliers. The Semantic factor is given by
\begin{equation}
        \mathtt{SF}(\bs{m}_i) = \varphi_g(\bs{m}_i) \varphi_p(\bs{m}_i)
\end{equation}

Differently, the geometric factor, $\mathtt{GF}(\bs{m}_i)$ combines the three other factors (Door Orientation, Robot Orientation, and Closeness), which are computed based on the geometry of the environment. During the search, it is important that the semantic planner makes the most optimal decisions, like when the SR is in a crossroads and there are multiple options of where to go. In situations like this, the Door Orientation factor aims to suggest which corridors are more likely to contain door signs. Based on the robot's orientation when it recognised the all past door signs, this factor estimates the best option. Similarly, the Robot Orientation factor estimates how the corridors of the environment are organised. It projects which corridors are more frequent to happend based on their orientation (e.g. vertical or horizontal). These two orientation-based factors are important to the semantic planner because they provide vital geometric information of the organisation of the building, e.g., in which corridors is expected to contain door signs. The goal of the last factor, Closeness, is to save SR's batery. Among the segments the planner has to choose to take the robot, it measures how close each one is to the robot's current position. The geometric factor multiplies the Robot and the Door Orientation factors, $\varphi_r(\cdot)$, $\varphi_o(\cdot)$ respectively, by the Closeness one, $\varphi_c(\cdot)$. The idea is that the further the segment is, the less it matters. Then, the outcome of these multiplications is summed to the $\varphi_c(\cdot)$. The geometric factor is given by
\begin{equation}
        \mathtt{GF}(\bs{m}_i) = \frac{\big(\varphi_o(\bs{m}_i) + \varphi_r(\bs{m}_i)\big)\varphi_c(\bs{m}_i) + \varphi_c(\bs{m}_i)}{3.0}.
        \label{eq:geometricfactor}
\end{equation}

Finally, in order to compute the best $\bs{m}_i \in \mathbf{C}$, i.e. the $\bs{m}_i$ that is more likely to contain the goal-door, each of them is submitted to the following equation
\begin{equation}
    \varphi(\bs{m}_i) = \mathtt{SF}(\bs{m}_i) \alpha + \mathtt{GF}(\bs{m}_i)(1.0 - \alpha). 
    \label{eq:finalEquation}
\end{equation}
Here, $\alpha$ is a threshold that controls the importance of the $\mathtt{SF}(\bs{m}_i)$ and $\mathtt{GF}(\bs{m}_i)$, and it ranges as $0 \leq \alpha \leq 1$. To estimate the candidade cell, $\bs{m}_i^*$, in which its $S(\bs{m}_i)$ is more likely to contain the goal-door, we do
\begin{equation}
    \bs{m}_i^* = \underset{\bs{m}_i \in \mathbf{C}}{\argmax}(\varphi(\bs{m}_i)). 
    \label{eq:maxfinalEquation}
\end{equation}


\subsection{Growing Direction factor}
\label{subsec:growingDirection}
Usually, doors sign of buildings are arranged in sequence and sorted (either in increasing or decreasing order). For example, the number of the first door sign in a corridor is smaller than the last one, or in the other way around. This characteristic can be inferred through the door sign sequence analysis. Imagine, for instance, that an SR is in a corridor where the number of the first door sign is larger than the goal-door one, and this corridor has an increasing door sign sequence. Hence, in terms of the Growing Direction factor, the SR should not consider this path as promising, since it is not very likely that its door sequence contains the goal-door. Therefore, the proposed Growing Direction factor, semantic information inferred from the door sign sequence, is beneficial to our NSOS system, given that it indicates the door signs organization in a segment. 

For each $\bs{m}_i \in \mathbf{C}$, the Growing Direction factor first calculates the angle in which the door sign sequence is increasing, $\theta_i\big(S(\bs{m}_i)\big)$. To determine it, all the detected door signs of the segment $S(\bs{m}_i)$ are considered, $L\big(S(\bs{m}_i)\big)$, as illustrated by Figure \ref{fig:doorsGroingAngle_a}. Then, for all possible pairs of two different door signs, one is larger than the other, the vector that connects them is computed. Figure \ref{fig:doorsGroingAngle_b} demonstrates an example for door sign number 1, and how the vectors are computed in pairs, such as (1,2), (1,3), (1,4), and (1,5). \textcolor{black}{As it shows, the door sign number 1 has four vectors, while the door sign number 4 has only one. To illustrate, if we align all these detected door signs in the centre of the corridor, as shown by Figure \ref{fig:doorsGroingAngle_c}, it would be easier to understand that the sum of vectors from Figure \ref{fig:doorsGroingAngle_b} and the final $\theta_i\big(S(\bs{m}_i)\big)$, indicate that the sequence increases to the right. To make this process even clearer, Figure \ref{fig:doorsGroingAngle_d} repeats the same procedure to the other door signs remaining, i.e., 2, 3, and 4}. Here, it is important to mention that the door signs, i.e., the yellow circles, were represented within the white area to help the explanation. In practice, they appear within the grey area, as illustrated in Figure~\ref{fig:imageProcessing}b.

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_empty.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_a}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_arrows.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_b}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_door_signs_linegedup.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_c}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/growing_direction_all_arrows.jpeg}
         \caption{}
         \label{fig:doorsGroingAngle_d}
     \end{subfigure}
     \caption[Demonstration of how the increasing angle $\theta_i\big(S(\bs{m}_i)\big)$ is computed in a segment]{\small Demonstration of how the increasing angle $\theta_i\big(S(\bs{m}_i)\big)$ is computed in a segment. All the detected door signs within the segment, (a), are considered to calculate the $\theta_i\big(S(\bs{m}_i)\big)$. The first step, (b), illustrates the vectors from door sign 1 to the other door signs, and it is easier to understand the effect of this vector calculation aligning all the door signs, (c). The final step of the vector computation, (d), shows all the vectors to all possible pairs.}
     \label{fig:doorsGroingAngle}
 \end{figure}

%\begin{figure}[!h]
%\centering
%(a)\hspace{5.5cm}(b)\\
%\includegraphics[width=.48\textwidth]{figs/growing_direction_empty.jpeg}
%\includegraphics[width=.48\textwidth]{figs/growing_direction_arrows.jpeg}
%\\(c)\hspace{5.5cm}(d)\\
%\includegraphics[width=.48\textwidth]{figs/growing_direction_door_signs_linegedup.jpeg}
%\includegraphics[width=.48\textwidth]{figs/growing_direction_all_arrows.jpeg}
%\caption{\textcolor{black}{Demonstration of how the increasing angle $\theta_i\big(S(\mathbf{c})\big)$ is computed in a segment. All the detected door signs within the segment, (a), are considered to calculate the $\theta_i\big(S(\mathbf{c})\big)$. The first step, (b), illustrates the vectors from door sign 1 to the other door signs, and it is easier to understand the effect of this vector calculation aligning all the door signs, (c). The final step of the vector computation, (d), shows all the vectors.}}
%\label{fig:doorsGroingAngle}
%\end{figure}

Figure~\ref{fig:groingDirection} illustrates a partial map from the simulator used in this chapter, and it helps to explain the importance of the Growing Direction. The robot has started at the intersection of three corridors, and it has chosen the corridor number 3, i.e., the one on the right. According to the robot's orientation in this corridor 3, the door sign sequence is considered increasing. Hence, in this current scenario, if the goal-door was 40, for instance, the Growing Direction factor would consider corridor 3 as promising. In contrast to this, the same corridor 3 would not be promising if the goal-door was 2, because the sequence only increases. It means that the distance from door sign 2 also increases as the SR continues in that corridor and the SR will not get closer to the goal-door. Besides these two examples, which help to understand how the Growing Direction factor behaves, a third case is important to mention. Imagine that the goal-door was 21. This factor would be high until the SR recognizes the door sign 20 in corridor 3. However, after that its value would decrease as the SR goes on, and the door sign sequence increases. Hence, in terms of only the Growing Direction factor, the robot should continue its search in either corridor 1 or 2. 

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_doorlabelsequences1.png}
         \caption{}
         \label{fig:all_doorlabelsequences1}
     \end{subfigure}\\
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_doorlabelsequences2.png}
         \caption{}
         \label{fig:all_doorlabelsequences2}
     \end{subfigure}\\
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_doorlabelsequences3.png}
         \caption{}
         \label{fig:all_doorlabelsequences3}
     \end{subfigure}
\caption[Partial 2D map of the environment to show the importance of Growing Direction factor]{Partial 2D map of the environment to show the importance of Growing Direction factor. All images represent the same part of the environment, but (a) shows the simple 2D grid map, (b) shows the visited region, and (c) shows the two segments of the map.}
\label{fig:groingDirection}
\end{figure}

One possible solution to deal with the aforementioned third case is to measure whether all the door signs within the sequence are smaller or bigger than the goal-door. Hence, the amount of door signs that are smaller or bigger than the goal-door are counted by the functions $SL\big(S(\bs{m}_i)\big)$ and $BL\big(S(\bs{m}_i)\big)$, respectively. The factor $\zeta(\cdot)$ measures the possibility of a segment to have door signs smaller or bigger than the goal-door, defined by 
\begin{equation}
\zeta\big(S(\bs{m}_i)\big) = \frac{\Big(SL\big(S(\bs{m}_i)\big) - BL\big(S(\bs{m}_i)\big)\Big)}{\max\Big(SL\big(S(\bs{m}_i)\big)+BL\big(S(\bs{m}_i)\big), w_g\Big)},
\label{eq:growingdirectionwg}
\end{equation} 
where $-1 \leq \zeta\big(S(\bs{m}_i)\big) \leq 1$. When $\zeta\big(S(\bs{m}_i)\big) = 1$, it means that in $S(\bs{m}_i)$ there are only bigger door signs than the goal-door. On the other hand, when $\zeta\big(S(\bs{m}_i)\big) = -1$, there are only smaller door signs than the goal-door. Lastly, when $\zeta\big(S(\bs{m}_i)\big) = 0$, both $SL(\cdot)$ and $BL(\cdot)$ are equal. $w_g$ is a threshold used to control the minimum amount of detected door signs are necessary to this equation reaches 1 or -1.

In addition, Growing Direction factor also considers $\theta_f(\bs{m}_i)$. It is the Voronoi angle at cell $\bs{m}_i$. The difference angle between $\theta_f(\bs{m}_i)$ and $\theta_i\big(S(\bs{m}_i)\big)$, measured by $\gamma(\theta_f(\bs{m}_i))$, indicates whether $\theta_f(\bs{m}_i)$ is pointing to the same direction than $\theta_i\big(S(\bs{m}_i)\big)$. Then,\\
\begin{equation}
\gamma(\theta_f(\bs{m}_i)) = 1.0 + \left | \frac{\theta_f(\bs{m}_i) - \theta_i\big(S(\bs{m}_i)\big)}{\pi} \right | (- 2.0),
\end{equation}
where $-1 \leq \gamma(\theta_f(\bs{m}_i)) \leq 1$. When $\gamma(\theta_f(\bs{m}_i)) = 1$, it means that $\theta_f(\bs{m}_i)$ and $\theta_i\big(S(\bs{m}_i)\big)$ are pointing at the same direction. On the contrary, when $\gamma(\theta_f(\bs{m}_i)) = -1$, it means that they are pointing to opposite directions. 

Now, the Growing Direction factor of a cell $\bs{m}_i$, $\varphi_g(\bs{m}_i)$, is defined as
\begin{equation}
\varphi_g(\bs{m}_i) = \frac{\zeta\big(S(\bs{m}_i)\big) \gamma\big(\theta_f(\bs{m}_i)\big) + 1.0}{2.0},    
\end{equation}
where $0 \leq \varphi_g(\bs{m}_i) \leq 1$. When $\varphi_g(\bs{m}_i) = -1$, it means that is less likely to reach the goal-door given how the door signs are set in $S(\bs{m}_i)$. Differently, when $\varphi_g(\bs{m}_i) = 1$, it means that is high likely. When $\varphi_g(\bs{m}_i) = 0$, it means that the Growing Direction factor is not sure about either the growing direction angle, or about the smaller and larger numbers. Hence, it cannot indicate whether $\bs{m}_i$ is a very likely frontier.
 
\subsection{Parity factor}
\label{subsec:parity}
This factor considers the characteristics of a door sign to be either \textit{even} or \textit{odd}, the kind of information that is not explicitly available in the environment. However, it can be easily inferred after the number recognition. The idea is to attribute a high probability to corridors that contain mostly door signs with the same parity as the goal-door. It is important to mention that this factor also considers the case in which a corridor contains both even and odd door signs since the probability is proportional to their respective amount.

To calculate the Parity factor, first it is necessary to count the amount of door signs from $S(\bs{m}_i)$ that have their parity alike or different than the goal-door. We use the functions $AL\big(S(\bs{m}_i)\big)$ and $DL\big(S(\bs{m}_i)\big)$ to count the alike and different parities, respectively. Then, for a cell $\bs{m}_i \in \mathbf{C}$, its Parity factor, $\varphi_p(\bs{m}_i)$, is given by
\begin{equation}
    \varphi_p(\bs{m}_i) = 0.5 + \frac{AL\big(S(\bs{m}_i)\big) - DL\big(S(\bs{m}_i)\big)}{\max\Big(AL\big(S(\bs{m}_i)\big) + DL\big(S(\bs{m}_i)\big), w_p\Big)}0.5
    \label{eq:paritywp}
\end{equation}
where $0 \leq \varphi_p(\bs{m}_i) \leq 1$, and $w_p$ is a threshold used to control the minimum amount of detected door signs that are necessary to this equation reaches 0 or 1. When $\varphi_p(\bs{m}_i) = 1$, it means that all the observed numbers from doors sign in the segment where the SR is have the same parity than the goal-door, whereas $\varphi_p(\bs{m}_i) = 0$ is the opposite. When $\varphi_p(\bs{m}_i) = 0.5$, it means that $AL(\cdot)$ and $DL(\cdot)$ are equal, and therefore is not possible to ensure the parity of the segment $S(\bs{m}_i)$.

\subsection{Robot and Door Orientation factors}
\label{subsec:robotDoorOrientation}
The SR moves through the environment, and it detects numbers from door signs as they are in its path. Usually, the position of doors in the environment follows a pattern that includes the possibility of existing doors only on horizontal or vertical corridors. Therefore, aiming to find the goal-door quickly, it is better to prioritize corridors in the same orientation than the already visited ones containing many doors. If the robot can prioritize the corridors in the same orientation, by consequence, its most common orientation will be an angle similar to these corridors.  

To calculate the Door Orientation factor, it is considered a history of the $\lambda_d$ most recent robot's orientations when a door sign was detected.
Figure~\ref{fig:all_orientation_robots} illustrates an example, in which the robot's poses from \textbf{c} to \textbf{k} would be saved. Based on this history, it is computed a histogram of such orientations. Each bin saves the percentage of each possible robot's orientation. Then, given the orientation of $\bs{m}_i$, $\theta_f(\bs{m}_i)$, it is consulted in the robot's orientation histogram the probability of finding a door sign considering such orientation, 
\begin{equation}
\label{eq:doorOrientation}
    \varphi_o(\bs{m}_i) = H_d[\theta_f(\bs{m}_i)],
\end{equation}
where $H_d[\cdot]$ is the door orientation histogram, and the Door Orientation factor is $0 \leq \varphi_o(\bs{m}_i) \leq 1$, in which 1 is 100\% and 0 is 0\%. The number of bins in the histogram $H_d[\cdot]$ defines its level of sampling. A $H_d[\cdot]$ with 360 bins provides a finer estimation than a $H_d[\cdot]$ with 45 bins. In our work, $H_d[\cdot]$ has maximum 179 bins, because we are not interested in saving the direction. Hence, if the robot's orientation is \ang{0} or \ang{180}, both will point to the same index in $H_d[\cdot]$ because they are in the horizontal line (although pointing to opposite directions).

The Door and Robot Orientation factors are very similar to each other. The difference between them is that the first one saves the robot's orientation only when a door sign has been recognized. Therefore, it prioritises the $\theta_f(\bs{m}_i)$ that has the highest $H_d[\theta_f(\bs{m}_i)]$, i.e. the orientation in which the robot has detected most of the door signs. On the other hand, the idea of the second one, the Robot Orientation factor, is to prioritize the $\theta_f(\bs{m}_i)$ that is most similar to the robot's orientation that is more frequent. It does not consider when the door signs were recognized. This factor makes the robot consider other paths that, despite not having door signs, may connect to other more promising ones.

As the robot moves through the environment, its $\lambda_r$ most recent orientations are saved, as shown by the robot's pose, from \textbf{a} to \textbf{o}, in Figure~\ref{fig:all_orientation_robots}. They are used to compute a histogram. Each histogram bin represents an angle and its percentage in the history of the robot's orientation. Given the calculated histogram, the $\theta_f(\bs{m}_i)$ is used as an index to get the probability of that angle, as presented by
\begin{equation}
    \varphi_r(\bs{m}_i) = H_r[\theta_f(\bs{m}_i)],
\end{equation}
where $H_r[\cdot]$ is the robot orientation histogram, and the Robot Orientation factor is $0 \leq \varphi_r(\bs{m}_i) \leq 1$. When $\varphi_r(\bs{m}_i) = 1$, it means that $\theta_f(\bs{m}_i)$ is an orientation that is equal to the unique robot's orientation saved. On the other hand, when $\varphi_r(\bs{m}_i) = 0$, it means that $\theta_f(\bs{m}_i)$ is an orientation that the robot did not do. %Similar to $H_d[\cdot]$, $H_r[\cdot]$ also may have different numbers of bins. As the robot's orientation is measured in the interval [$\pi$, -$\pi$], in our work $H_r[\cdot]$ may have maximum only 179 bins (the $H_r[\cdot]$ does not represent the direction, and hence, \ang{0} and \ang{180} are stored at the same bin in $H_r[\cdot]$).

The scenario in Figure~\ref{fig:all_orientation_angles} illustrates the importance of both Robot and Door Orientation factors. The robot starts at the pose \textbf{a} in Figure~\ref{fig:all_orientation_robots}, and it moves forward until pose \textbf{p}. Then, it finds a second intersection between points 3 and 4, and it has to decide which one it should take. At this moment, the robot has detected door signs from pose \textbf{c} to \textbf{k}, with robot's orientation mostly at \ang{0} as the reference shown in Figure~\ref{fig:all_orientation_robots}. Then, $H_d[0] = 100\%$. Besides, the robot has moved most of the time heading \ang{0}, as illustrated by the 12 poses (from \textbf{a} to \textbf{l}) in Figure~\ref{fig:all_orientation_robots}, against the four poses (from \textbf{m} to \textbf{p}) heading approximately \ang{270}. Then, $H_r[0] > H_r[270]$. Hence, when the semantic planner has to decide where to go, among the four available points, the 3 and 4 will have higher priorities due their proximity to the robot's pose. Besides, point 3 will have the highest priority among the two because of its orientation, which is \ang{180}. In this case, $H_d[0] = 100\%$ (recall that \ang{0} and \ang{180} point to the same index in $H_d[\cdot]$) while point 4 is $H_d[90] = 0\%$ (converting \ang{270} to the range [\ang{0},\ang{180}] of $H_d[\cdot]$). Therefore, $H_d[\cdot]$ suggests that point 3 is the most promising one considering the organisation of the environment that has been observed by the robot. 

\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_orientation_robots.png}
         \caption{}
         \label{fig:all_orientation_robots}
     \end{subfigure}\\
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_orientation_points.png}
         \caption{}
         \label{fig:all_orientation_points}
     \end{subfigure}
\caption[Partial 2D map of the environment to explain the robot and door orientation factors]{Partial 2D map of the environment to show the importance of the Robot and Door Orientation factors. (a) shows the robot's orientation during its trajectory and (b) the orientation of each point.}
\label{fig:all_orientation_angles}
\end{figure}


%\begin{figure}[!h]
%     \centering
%     \begin{subfigure}[b]{0.7\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/all_direction1.png}
%         \caption{}
%         \label{fig:all_direction1}
%     \end{subfigure}\\
%     \begin{subfigure}[b]{0.7\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/all_direction2.png}
%         \caption{}
%         \label{fig:all_direction2}
%     \end{subfigure}\\
%     \begin{subfigure}[b]{0.7\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/all_direction3.png}
%         \caption{}
%         \label{fig:all_direction3}
%     \end{subfigure}
%\caption[Partial 2D map of the environment to explain the robot and door orientation factors]{Partial 2D map of the environment to show the importance of the Robot and Door Orientation factors. All images represent the same part of the environment, but (a) shows the simple 2D grid map, (b) shows the visited region, and (c) shows the five segments of the map.}
%\label{fig:orientationFactors}
%\end{figure}

\subsection{Closeness factor}
\label{subsubsec:distance}
The fifth factor considered by our Semantic Planner is the distance between the robot cell $\bs{m}_r$ and each $\bs{m}_i \in \mathbf{C}$, i.e. the smallest number of Voronoi cells that connects each pair of $(\bs{m}_r, \bs{m}_i)$. Its goal is to guide the robot towards the closest $\bs{m}_i$, instead of spending battery and time going to the farthest one. Take the Figure~\ref{fig:distanceFactor} as an example, and suppose that the goal-door was 71. The robot has started at the intersection between corridors 1 and 2, and it has moved to the right corridor, Figure~\ref{fig:all_distance1}. After a few minutes, guided by the Robot and Door Orientation factor, it has chosen to continue the searching on corridor 3. Even though this corridor has the same parity as the goal-door (both are odd), the Growing factor indicates that corridor 3 is not promising. Therefore, the robot should continue the searching in one of the other three options, corridors 1, 2, or 4. The Closeness factor is responsible for indicating the closest option to the robot, given by the sum of green cells that connect the robot's current position and each red point near the numbers 1, 2, and 4, as shown in Figure~\ref{fig:all_distance2}. 

\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_distance1.png}
         \caption{}
         \label{fig:all_distance1}
     \end{subfigure}\\
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_distance2.png}
         \caption{}
         \label{fig:all_distance2}
     \end{subfigure}\\
     \begin{subfigure}[b]{0.7\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/all_distance3.png}
         \caption{}
         \label{fig:all_distance3}
     \end{subfigure}
\caption[Partial 2D map of the environment to explain the distance factor.]{Partial 2D map of the environment to show the importance of the Closeness factor. All images represent the same part of the environment, but (a) shows the simple 2D grid map, (b) shows the visited region, and (c) shows the six segments of the map.}
\label{fig:distanceFactor}
\end{figure}

The first step of the Closeness factor calculation is to find the smallest proximity $\bs{m}_j^{\ll}$ between $\bs{m}_r$ and all $\bs{m}_i \in \mathbf{C}$. It only considers the Voronoi cells in $\bs{m}$, in which one cell is considered as one to the proximity sum. It is given by
\begin{equation}
    \bs{m}_j^{\ll} = \underset{\bs{m}_i \in \mathbf{C}}\argmin\big(D(\bs{m}_r, \bs{m}_i)\big)
\end{equation}
where $D(\cdot, \cdot)$ is the function that counts the number of cells between two other specific cells. In this factor, only Voronoi cells are counted, regardless they are within mapped or unknown regions.

The idea is that the Closeness factor of $\bs{m}_i$, $\varphi_c(\bs{m}_i)$, should be high to small proximities, and low to the big ones, i.e., give more preference to $\bs{m}_i$ that is closer to the robot. Then
\begin{equation}
    \varphi_c(\bs{m}_i) = 1.0 - \Big(1.0 - \frac{D(\bs{m}_r, \bs{m}_j^{\ll})}{D(\bs{m}_r, \bs{m}_i)}\Big)^4,
\end{equation}
where $0 \leq \varphi_c(\bs{m}_i) \leq 1$, in which $\varphi_c(\bs{m}_i) = 1$ means that $D(\bs{m}_r, \bs{m}_i)$ is equal to $D(\bs{m}_r, \bs{m}_j^{\ll})$, and $\varphi_c(\bs{m}_i) = 0$ means that $D(\bs{m}_r, \bs{m}_i)$ is so high that makes the division be around zero. 

\section{Experiments and Results} 
\label{sec:experimentsResults}
This section presents the experiments carried out in simulation and in the real world. Section \ref{subsec:experimentSetup} explains the software setup used in our simulated experiments, as well as the differences between the physical and simulated experiments. Section \ref{subsec:comparativeMethods} presents the results of the simulation phase, comparing the performance of our NSOS system and an entirely geometric OS system, called \textit{Greedy}. Four different maps were considered in this comparison. Section~\ref{subsec:humanresults} introduces a second type of comparison, in which these two initial OS systems, ours and the Greedy one, are compared to human participants teleoperating the robot in the simulation setup while performing the searching task. Finally, Section~\ref{subsec:realResults} demonstrates how our NSOS system performs in the physical world, as well as the information about where this test was performed. 

It is also important to report the parameters used by our system throughout all the experiments presented below, either in simulation or in the real world. Both $\textit{w}_g$ and $\textit{w}_p$ are set to eight. This means that in Equations \ref{eq:growingdirectionwg} and \ref{eq:paritywp}, respectively, the closer or higher to eight the number of detected door signs is, the more important the Growing Direction and Parity factors become. The number eight was chosen to balance the importance of the factors since a small number would make them important soon in the search process, and a large number would play the opposite role. In addition to these two parameters, the Robot and Door orientation factors also have some parameters. The size of the $H_d[\cdot]$ is four, which is the outcome of dividing the range of [\ang{0},\ang{179}] by \ang{45}. It means our approach considers the robot's orientations when detecting a door sign in groups of \ang{45} (e.g., if the robot detects a door sign and its orientation is \ang{42}, $H_d[0]$ is incremented). For the histogram $H_d[\cdot]$, we consider the past $6.000$ orientations, as this sensor reading is noisy. For the case of $H_r[\cdot]$, we assume a finer setup since the robot may be in a different orientation in the range of [\ang{0},\ang{359}]. The size of $H_d[\cdot]$ is 18, and we consider the past $600.000$ readings due to our high reading rate from the robot's orientation, the presence of noise in the data, and to reduce the impact of an unexpected turning that may happen.

\subsection{Simulated Experiment Setup}
\label{subsec:experimentSetup}
The setup of the simulated experiments is represented in Figure~\ref{fig:simulators}. The MobileSim simulates a Pioneer 3-DX robot equipped with a \ang{180}~Laser, providing its odometry information and its laser sensor readings, Figure~\ref{fig:simulators}a. However, MobileSim does not provide information from door signs, which is vital for our experiments. Therefore, we developed a door simulator (DS) to mimic both the two RGB cameras that are embedded in the physical robot and the Image Processing module that recognizes the numbers, Figure~\ref{fig:simulators}c. DS provides the number of door signs and their positions in the world when they are within the robot's FoV. Then, the final setup combines the MobileSim to read the robot's information and our DS to provide the door signs information, as illustrated in Figure~\ref{fig:simulators}. %As the robot moves through the map in the MobileSim, the DS provides the number and the pose of the door labels if the robot is in front of one.

\begin{figure*}[!h]
\centering
\includegraphics[width=1\textwidth]{figs/drawing2.jpeg}\\
(a) \hspace{4.5cm} (b) \hspace{4.5cm} (c)
\caption[Software setup used in the simulated experiments]{Software setup used in the simulated experiments. It shows the MobileSim in (a), (b) represents the robot's and door signs information as input to our NSOS system that returns the robot's next movements, and (c) is the door signs map as ground-truth in the Door simulator. Both (a) and (c) represent the same position on the map.}
\label{fig:simulators}
\end{figure*}

The first evaluation of our NSOS system was made through the comparison with the Greedy OS system in simulated indoor environments. The experiments considered four different scenarios. Table \ref{tab:scenarios} and Figure~\ref{fig:simulationMaps} present the details of the scenarios. The four scenarios vary considerably regarding the number of door signs and how they are set, the size of the buildings, and the orientation of the corridor. The \textit{Normal} and \textit{Inverse} were made aiming to test the OS systems in scenarios with many long corridors intersecting each other, where the OS systems are forced to make decisions very often. Due to the high number of door signs in both scenarios, four of them were chosen as goal-doors for the tests, one in each horizontal corridor. Their difference is that \textit{Normal}, Figure~\ref{fig:simulationMaps}a, has its door signs sequence increasing from the middle to the borders. The \textit{Inverse}, Figure~\ref{fig:simulationMaps}b, is in the other way around. This way, we can test the performance of our NSOS system in different door signs arrangements. The \textit{Hotel} is the third scenario used in the experiments, and it represents the third and fourth floors of the Hotel Pennsylvania~\cite{McKim1919Hotel} located in New York. It has the highest amount of door signs, along with a large environment containing many door signs and long corridors, Figure~\ref{fig:simulationMaps}c. The \textit{Hotel} scenario aims to test the OS systems in terms of how they perform when there is a large number of door signs in the environment. A bad choice in \textit{Hotel} may cause a long run that will not lead to the goal-doors. The fourth scenario is from a public dataset called KTH Campus\footnote{It was used the left building from the floor plan identified as 0510028829\_A30\textendash00\textendash07, A0043015. The dataset can be found at \url{http://www.csc.kth.se/~aydemir/KTH_CampusValhallavagen_Floorplan_Dataset.tar.bz2}}, Figure~\ref{fig:simulationMaps}d, that contains more than 38,000 rooms in total, considering the many floor plans from different buildings~\cite{Aydemir2012What}. Even though the particular floor plan chosen for this test, called \textit{KTH} scenario, has the lowest amount of door signs compared to the three other scenarios used in the tests, it presents corridors in a different orientations, i.e., not only in horizontal or vertical. All tests in the simulation were carried out on a laptop with 8GB RAM and processor \textit{i7}. 

\begin{table}[h]
\centering
\caption{Different scenarios used on our simulated tests.}
\label{tab:scenarios}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Name}  & \textbf{\# of Door signs} & \textbf{Goal-doors}        \\ \hline
\textit{Normal}           & 113                        & 54, 55, 111, 124  \\ \hline
\textit{Inverse}           & 116                        & 54, 55, 111, 124  \\ \hline
\textit{Hotel}             & 124                        & 76, 135, 148, 185 \\ \hline
\textit{KTH}             & 47                        & 756 \\ \hline
\end{tabular}
\end{table}

\begin{figure*}[t!]
\centering
     \begin{subfigure}[b]{0.47\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/map_normal_redcircles.jpeg}
         \caption{}
         \label{fig:realResults_a}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.47\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/map_inverse_redcircles.jpeg}
         \caption{}
         \label{fig:realResults_b}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.52\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/hotel2_PAPER.jpeg}
         \vspace{2cm}         
         \caption{}
         \label{fig:realResults_c}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.47\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/KTH_CampusValhallava_finished.jpeg}
         \caption{}
         \label{fig:realResults_d}
     \end{subfigure}

%\includegraphics[width=.4\textwidth]{figs/map_normal_redcircles.jpeg} \\ 
%(a) \\
%\includegraphics[width=.4\textwidth]{figs/map_inverse_redcircles.jpeg} \\ 
%(b) \\
%\includegraphics[width=.43\textwidth]{figs/hotel2_PAPER.jpeg} \\ 
%(c) \\
%\includegraphics[width=.43\textwidth]{figs/KTH_CampusValhallava_finished.jpeg}\\
%(d)
\caption[The four maps used in the simulated experiments.]{The four maps used in the simulated experiments. The green squares represent the position where the robot has started, and the red circles highlight the goal-doors. The maps are (a)~\textit{Normal}, (b)~\textit{Inverse}, (c)~\textit{Hotel}, and (d)~\textit{KTH}.} 
\label{fig:simulationMaps}
\end{figure*}

\subsection{Semantic and Greedy Object Search systems}
\label{subsec:comparativeMethods}
Our NSOS system was early introduced in Sections~\ref{sec:method} and~\ref{sec:semanticPlanner}. In this section, the performance of our system is compared to the Greedy OS system, which has the exact same basis presented in Section~\ref{sec:method}, but its planner is composed only by the geometric factor from Equation~\ref{eq:finalEquation}, i.e. the Equation~\ref{eq:finalEquation} with $\alpha = 0$. Therefore, both systems only differ by their planners, which are responsible for the reasoning over their inputs. In summary, the Greedy OS system searches for goal-doors based on the nearest frontier, whereas our NSOS system considers environmental information to make smarter decisions.

Both systems were tested using the same simulation setup. For the four scenarios, the door signs shown in Table~\ref{tab:scenarios} were set as goal-doors. They were chosen to cover as many corridors of the scenarios as possible. For each goal-door, both systems have been run ten times each in these experiments, so we have statistically significant results. For every test, it was measured, in meters, the distance traveled by the robot from its initial position until it finds the goal-door. Since we consider the distance traveled as the search cost, the shorter the distance, the better is the system's performance. Even though we have not measured and presented the search cost in terms of time, it is important to mention that both our NSOS system and the Greedy one moved the robot with the same velocity, and at no moment the robot stood still, wasting time. Therefore, the system that provides the shortest distance traveled is also the fastest. Throughout the tests presented in this Section, in Equation~\ref{eq:corridorNotcorridor} we used $\delta = 2$. This parameter mean that the map of the environment has been segmented into two tyoes, a corridor and not a corridor, as shown in Figure~\ref{fig:2dmapSegmentation_c}. %\alpha

Tables~\ref{fig:resultsNormal},~\ref{fig:resultsInverse},~\ref{fig:resultsHotel}, and~\ref{fig:resultsKTH} present the results of both systems in each scenario in the simulated tests, \textit{Normal}, \textit{Inverse}, \textit{Hotel}, and \textit{KTH}, respectively. The colorful columns represent the results achieved by the tested OS systems. The first one is the result from the Greedy OS system, and the other three are from our semantic one. In the greedy column, the value $0,00\%$ means that $\alpha = 0\%$ in Equation~\ref{eq:finalEquation}, and hence, the planner becomes fully geometric. In the semantic columns, the same value ranges from $80,0\%$ until $100,0\%$, which means that the $\alpha$ ranges from $0.80$ until $1.0$ in Equation~\ref{eq:finalEquation}. Hence, it changes the importance of the semantic factor in that equation. Our NSOS system was also tested with $\alpha$ ranging from $0.5$ up to $0.7$, but the results were not significant and not presented in the tables. The rows of the tables correspond to the two OS systems' performance while searching for each goal-door. Each system's performance for every goal-door was evaluated in terms of Median, Average, Standard Deviation, Minimum, and Maximum distances. It is also important to highlight that within a row, the color of the table cells ranges from green to red. Green represents the cell with the smallest value within a row, and red represents the largest one.

The results in Table~\ref{fig:resultsNormal}, \textit{Normal} scenario, and in Table~\ref{fig:resultsInverse}, \textit{Inverse} scenario, are similar in terms of which column has the most red cells. In both cases, our NSOS system has a better performance than the greedy one,  as most of the green cells are within the semantic columns, mainly when $\alpha = 80.0\%$ and $\alpha = 90.0\%$. In one of its ten runs for such map, the Greedy OS system made a sequence of decisions that lead it to find the goal-door 54 with the shortest distance ($82.19~m$ in Table~\ref{fig:resultsNormal}). This low result probably contributed to the lowest average being achieved by it, $121.94~m$ in the same table. However, it is also important to highlight that the standard deviation for the Greedy system for that goal-door is the highest one, $41.59~m$. It means that it did not find the goal-door 54 traveling the shortest distance every test. A similar behaviour for this system can be seen when searching for goal-door 124, in which the minimum traveled distance was achieved by the Greedy system, $49.56~m$, but not for all the ten runs (its standard deviation for this goal-door was the highest, $70.09~m$). In Table~\ref{fig:resultsInverse}, the lowest average and minimum of the goal-door 111 are from the greedy OS system, but again its standard deviation is the highest. It means that the ten tests of the greedy system vary considerably, as shown by the difference between its minimum and maximum values, $61.86~m$ and $278.57~m$, respectively. On the other hand, the standard deviation within the semantic columns is lower, meaning our semantic system has a constant behavior during the searches. It guides the robot through a similar path through the ten runs, making the same decisions in different executions. 

\begin{table}[h!]
\centering
  \caption[Results of the greedy and our NSOS systems in the \textit{Normal} scenario.]{Results of the greedy and our NSOS systems in the \textit{Normal} scenario. All the results are shown in meters.}
\label{fig:resultsNormal}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_NORMAL_NEW_T.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption[Results of the greedy and our NSOS systems in the \textit{Inverse} scenario.]{Results of the greedy and our NSOS systems in the \textit{Inverse} scenario. All the results are shown in meters.}
\label{fig:resultsInverse}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_INVERSE_NEW_T.eps}
\end{table}

The Tables~\ref{fig:resultsHotel} and~\ref{fig:resultsKTH}, from \textit{Hotel} and \textit{KTH}, present similar results than the two previous tables in terms of the greedy column having most of the red cells. Besides, in general our NSOS system has better performance than the greedy system. Both tables also show that our proposed system is efficient in real scenarios. Even though the $\alpha = 100.00\%$ column within the semantic columns presents satisfying results, mainly in Table~\ref{fig:resultsKTH}, a purely semantic OS system is not always suitable for searching tasks. The geometric factor in Equation~\ref{eq:finalEquation} is essential and, combined with the semantic factor, may provide the best results. The Greedy system presents good results for all the four goal-doors in terms of minimum traveled distance. Again, as this system does not repeat the sequence of decisions during the search over all the ten runs, sometimes it is lucky enough to make a sequence of decisions that lead it to find the goal-door very quickly. The lack of consistency and robustness on Greedy's performance is demonstrated by its poor results in terms of standard deviation, being the worst of all for all goal-doors in Tables~\ref{fig:resultsHotel} and~\ref{fig:resultsKTH}.

\begin{table}[!h]
\centering
  \caption[Results of the greedy and our NSOS systems in the \textit{Hotel} scenario.]{Results of the greedy and our NSOS systems in the \textit{Hotel} scenario. All the results are shown in meters.}
\label{fig:resultsHotel}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_HOTEL_NEW_T.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption[Results of the greedy and our NSOS systems in the \textit{KTH} scenario.]{Results of the greedy and our NSOS systems in the \textit{KTH} scenario. All the results are shown in meters.}
\label{fig:resultsKTH}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_KTH_NEW_T.eps}
\end{table}

Besides the previous analysis, the optimal solution for each scenario was also measured. It is the shortest path between the starting position, green squares, and a goal-door, red circles, in Figure~\ref{fig:simulationMaps}. The Tables~\ref{fig:shortestNormal}, ~\ref{fig:shortestInverse}, ~\ref{fig:shortestHotel}, and~\ref{fig:shortestKTH} present the optimal solution to each goal-door, as well as the average and standard deviation with each OS system from Tables~\ref{fig:resultsNormal}, ~\ref{fig:resultsInverse}, ~\ref{fig:resultsHotel}, and~\ref{fig:resultsKTH}. 

\begin{table}[!h]
\centering
  \caption[The average, standard deviations, and shortest path from the \textit{Normal} scenario]{The average and the standard deviations from the \textit{Normal} scenario, Table~\ref{fig:resultsNormal}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are shown in meters.}
\label{fig:shortestNormal}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_NORMAL_SHORTEST_NEW_T.eps}
\end{table}

In general, the difference between the optimal solution of each goal-door and the averages in the same line is larger for greedy system results than those from our NSOS system. For example, the optimal solution for the goal-door number 124 in Table~\ref{fig:shortestNormal} is $33.16~m$. The average of the Greedy system for the goal-door 124 is $148.5~m \pm 70.08~m$, which is almost five times larger than the shortest path. The results of our NSOS system for this same goal-door, in the worst case, is $67.04~m \pm 17.08~m$, which is just two times larger. 

The same analysis can be made for other goal-doors in other scenarios, Tables~\ref{fig:shortestInverse} and~\ref{fig:shortestHotel}. Besides this analysis, it is also possible to measure how large the averages are compared to their optimal solution. For all goal-doors and systems tested in the simulation experiments, Table~\ref{fig:percentageALL} shows how many times, in percentage, the averages are larger compared to the optimal solutions. For the case of our NSOS system from Tables~\ref{fig:shortestNormal},~\ref{fig:shortestInverse}, and~\ref{fig:shortestHotel}, it is considered the lowest average between the ones from $\alpha = 80.0\%, \alpha = 90.0\%$, and $\alpha = 100.0\%$.

\begin{table}[!h]
\centering
  \caption[The average, standard deviations, and shortest path from the \textit{Inverse} scenario]{The average and the standard deviations from the \textit{Inverse} scenario, Table~\ref{fig:resultsInverse}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are in meters.}
\label{fig:shortestInverse}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_INVERSE_SHORTEST_NEW_T.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption[The average, standard deviations, and shortest path from the \textit{Hotel} scenario]{The average and the standard deviations from the \textit{Hotel} scenario, Table~\ref{fig:resultsHotel}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are in meters.}
\label{fig:shortestHotel}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_HOTEL_SHORTEST_NEW_T.eps}
\end{table}

\begin{table}[!h]
\centering
  \caption[The average, standard deviations, and shortest path from the \textit{KTH} scenario]{The average and the standard deviations from the \textit{KTH} scenario, Table~\ref{fig:resultsKTH}, and the optimal solution (shortest path) between each goal-door and the starting position. All the results are in meters.}
\label{fig:shortestKTH}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_KTH_SHORTEST_NEW_T.eps}
\end{table}



To compute the percentages presented in Table~\ref{fig:percentageALL}, it is considered the optimal solution of each goal-door for each scenario as $100\%$. Hence, if the average is larger than the shortest path, it will be higher than $100\%$,  as in the case of the goal-door 54, scenario \textit{Normal}. The greedy system is approximately seven times larger than the optimal solution, i.e., $709.39\%$. 

In Table~\ref{fig:percentageALL}, most of the lowest percentages are within the semantic rows. There are few goal-doors in which the greedy system presents the lowest rate. That is the case of goal-door 54 of the \textit{Normal} scenario, and the 111 of the \textit{Inverse}. However, even though the greedy system presents low values, the values from the semantic system to the same goal-doors are similar. On the contrary, analyzing the goal-door 54 of the \textit{Inverse} scenario, for instance, the value from the greedy system is almost four times larger. 

\begin{table}[!h]
\centering
  \caption[Comparison of the optimal solution (shortest path) of each goal-door from each scenario]{Comparison of the optimal solution (shortest path) of each goal-door from each scenario, with the averages from Tables~\ref{fig:resultsNormal}, ~\ref{fig:resultsInverse}, and~\ref{fig:resultsHotel}. The shortest path is equivalent to $100\%$, and the figure shows how large the averages are in comparison with the optimal solution.}
\label{fig:percentageALL}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_COMPARISON_PERCENTAGE_NEW_T.eps}
\end{table}

The two OS systems were submitted to search for one goal-door at time in all scenarios. Hence, we do not have their results as if they were searching for a sequence of goal-doors in a single attempt. However, we can speculate their performance in such conditions, based on their results in the previous experiments. In general, our NSOS system finds the first goal quickly, demonstrating that it is efficient even when there is no information available \textit{a priori} about the environment. After finding the first goal-door, NSOS will be partially aware about the organisation of the environment, which may give it advantages for more accurate searches in the future. On the other hand, in most of the cases the Greedy system wastes too much robot's resources to find the first goal-door of the sequence. In this process, it may discover the next goal-doors of the sequence and improve its performance a little bit. However, if the next goal-doors were not found during the first search, its performance will be even worse than our NSOS sytem in such a scenario. 

\subsection{Human participants performance in object searching task}
\label{subsec:humanresults}
The results presented in Table~\ref{fig:percentageALL} illustrate how many times, in percentage, the results of the greedy and our NSOS systems are larger than the optimal solution. Some results from the semantic system are two, three, or even four times larger, whereas the greedy system provides results that are up to 12 times larger than the optimal solution for the scenario \textit{Inverse} and goal-door 54. 

Given only these high percentages, it seems that both systems are not suitable for the task of finding a target door sign in an unknown environment based on text information as visual cues. However, it is important to highlight that this task is challenging because the environment is unknown, and there is no way of planning an optimal path a priori. This section illustrates the difficulty level of the searching task by presenting an experiment in which human participants were invited to perform the searching while piloting the robot in the simulator presented in Section~\ref{subsec:experimentSetup}. This experiment measured (in meters) the distance traveled by the robot from the initial position until the goal-doors. The distance traveled is the search cost used in this work, and hence, the shorter the distance, the better is the system performance.

Instead of using the planner of either our semantic or the greedy OS system to find the goal-door task, ten human participants were invited to teleoperate the robot in the simulation setup to perform the same role as our semantic planner. The participants were presented to the searching task beforehand, with a time to get familiar with the robot control system and our simulation setup. This experiment aimed to measure human performance in the same setup as the other tested system to show whether human reasoning provides better results than our NSOS system in the same conditions. Therefore, the humans controlled the robot in the same simulator and graphical interface as the two OS systems, as shown by Figure~\ref{fig:2dmapSegmentation_a}. The difference of this experiment to the one from Section~\ref{subsec:comparativeMethods} is how the choices are made. In this case, the participants must choose where the robot must go, playing the planner role to choose the path.

In this experiment, only two scenarios were tested, with one goal-door each. The \textit{Normal} and \textit{Inverse} scenarios are certainly similar, differing only on how the door signs are arranged. Given that humans can memorize what they have seen, it would be unfair to submit them to two similar scenarios or more than one goal-door for the same scenario. Therefore, the \textit{Normal} and \textit{Hotel} scenarios were used for this experiment. Door signs picked as the target were $111$ to \textit{Normal} and $148$ to \textit{Hotel}. They are not too far nor too near to the robot's initial position. Hence, the participants would have to explore at least a small part of both scenarios. Throughout all the tests, the only data considered for the evaluation was the traveled distance. 

Table~\ref{fig:humansresults} summarises the analysis of the ten participants. Besides, it also compares human performance to the greedy and our NSOS systems. As in the previous tables, green represents the cells with the smallest value within a row, and red represents the largest. As can be seen, our system presents a smaller average in both goal-doors, with the lowest indices compared to the others. The minimum traveled distance for the goal-door $148$ is the only case where our NSOS system does not have the lowest result. For that result, one of the ten participants made some decisions that luckly brought them to the goal-door with the shortest traveled distance. The second shortest traveled distance from the participants was $199.23~m$, a result two times larger than the $75.23~m$ shown in Table~\ref{fig:humansresults}, and which supports that humans do not have a regular and efficient performance.

\begin{table}[!h]
\centering
  \caption[Human performance to the problem of OS system.]{Human performance to the problem of OS system. It is compared to the greedy and semantic systems, in which all of them had to find two goal-doors, one in each scenario. The results are presented in meters.}
\label{fig:humansresults}
  \includegraphics[width=.5\textwidth]{figs/RESULTS_HUMANS_NEW_T.eps}
\end{table}

In contrast to our NSOS system, the greedy one presents the worst results for both goal-doors, which confirms the previous results presented in Section~\ref{subsec:comparativeMethods}. It is also worth mentioning the high standard deviation of the Human participants for both goal-doors. It shows that they had different performances, in which some had a shorter traveled distance than others. Some participants did not follow the same pattern when making decisions throughout their run. At the end of their participation, they have reported that they did not have an efficient strategy to search for the target, and no reasoning was made based on the door signs. One specific participant forgot, for only a few seconds, the goal-door for the \textit{Hotel} scenario. The participant mentioned that as soon as they realized they could not remember de number, they look at a paper that was in front of them to read it again. The human eye has a wider field of view than the two cameras used in this experiment setup, which provides advantages to humans when searching for objects in an unknown environment. However, as previously mentioned, the goal of this experiment was to test humans as the decision-maker within the system. That is why they used the same software setup for the experiments as our semantic and greedy systems. 

Besides the lowest average from the NSOS system, there are other advantages compared to the participants' results. Its slight standard deviation means that the same decisions were taken in all the ten test repetitions, which suggests that our system does not make random decisions. On the other hand, the same does not apply to the Human results, which means that every participant had their particular reasoning to make a decision. Hence, some participants are more efficient than others in this OS task. Besides the standard deviation, another advantage is that robots are not disturbed by other moving objects or agents in the environment. Therefore, they can focus on the task, and they do not forget the goal-door, what happened to one of the humans during the experiment. 


\subsection{Experiments Using a Physical Robot}
\label{subsec:realResults}
The experiment with a physical robot was performed in one of the buildings of the Federal University of Rio Grande do Sul, Brazil, where the Phi Robotics Research Lab is located. Figure~\ref{fig:realMap} shows how this building is organised, as well as the rooms and their door signs. The robot used in this experiment is a Pioneer 3DX from MobileRobots, which is equipped with a Lidar laser scan of \ang{180}~and two RGB cameras, as shown in Figure~\ref{fig:robotSetup}.

\begin{figure}[h]
\centering
  \includegraphics[width=.75\textwidth]{figs/real_environment_map.jpeg}
  \caption[Map used in the experiments with the real robot.]{Map used in the experiments with the real robot. It is the building where the Phi Robotics Research Lab is located at the Federal University of Rio Grande do Sul, Brazil. The green square represents the position where the robot starts, and the red circle highlight the goal-door.}
\label{fig:realMap}
\end{figure}

\begin{figure}[h]
\centering
  \includegraphics[width=.5\textwidth]{figs/real_scenario_labeled2.jpeg}
  \caption[The Pioneer 3DX robot used in the real environment experiment.]{The Pioneer 3DX robot used in the real environment experiment, as well as the two embedded cameras. The door signs of the environment are also depicted on this figure.}
\label{fig:robotSetup}
\end{figure}

This experiment aims to demonstrate that our NSOS system works in physical scenarios, meaning that the robot should be able to find the target goal-door traveling the shortest distance as possible. %Even though the Image Processing module that detects the text in images is not part of the contribution of this paper, we also aim to test its performance as part of our approach. The simulation tests do not use this module to recognize the door labels. Therefore, this is the opportunity to measure its impact on the other parts of our approach.
For this experiment, goal-door 232 is chosen as the target, which is located on the left side of the initial position, the green square in Figure~\ref{fig:realMap}. Even though it was at the same corridor as the initial position, the experiment setup is suitable to prove that our system can reason over the detected door signs. From the initial position, the robot can turn to the left or the right. If it decides the left direction, it will find the goal-door quicker, but the other option would take it to the opposite side. For this case, as soon as a few door signs have been detected, our semantic system would be able to reason over them and infer that this direction is not promising. Hence, it should change to the opposite direction. Therefore, this would show that our semantic system uses the door signs to find the goal-door efficiently. 

%\begin{figure*}[!h]
%\centering
%  \includegraphics[width=.45\textwidth]{figs/1_Real_Exploration_11.eps}
%  \includegraphics[width=.45\textwidth]{figs/2_Real_Exploration_22.eps}\\
%  \includegraphics[width=.45\textwidth]{figs/3_Real_Exploration_33.eps}
%  \includegraphics[width=.45\textwidth]{figs/4_Real_Exploration_44.eps}\\
%  \includegraphics[width=.45\textwidth]{figs/5_Real_Exploration_55.eps}
%  \includegraphics[width=.45\textwidth]{figs/6_Real_Exploration_66.eps}
%  \caption{Step-by-step of the performance of our semantic AVS system in the physical environment. \textit{a} shows the initial position, where the robot has started the searching, whereas \textit{f} shows the final step when the robot has found the goal-door 232.}
%\label{fig:realResults}
%\end{figure*}

When submitted to finding the goal-door 232 in a physical environment, the performance of our proposed semantic OS system was similar to the situation described above. Figure~\ref{fig:realResults} depicts six steps of the system, and all of them show important moments for the searching. In Figure~\ref{fig:realResults_aa}, the robot just had finished one complete rotation to map its surroundings, and then it detected the door sign 240. In Figure~\ref{fig:realResults_bb}, the robot had chosen to turn right, where the door sign 242 has been found. This figure demonstrates how our planner decides on changing: \textit{i}) the door signs 240 and 242 were recognized in an increasing sequence, and it means that as the robot goes forward, the distance from the goal-door increases; \textit{ii}) the robot is between two frontiers, and even though the robot is closer to the one that is in front of it, the other is not that far from it; \textit{iii}) the first two door signs were found in a horizontal corridor, and so far, the horizontal corridors are the more likely ones to contain other door signs. Combining all this information, the decision is that the frontier that the robot is following is less promising than the other one that is behind it. In Figure~\ref{fig:realResults_cc}, the robot has changed its orientation to the opposite one. The door sign 238 was recognized, which supports the orientation change. In Figure~\ref{fig:realResults_dd}, as the robot has not recognized any other door sign that contradicts its decision, the searching continues towards the left direction. In Figure~\ref{fig:realResults_ee}, it recognizes the door sign 234, which indicates a decreasing sequence towards the goal-door. Finally, in Figure~\ref{fig:realResults_ff}, the robot detects the goal-door 232, and the robot finishes the searching. %Unfortunately, due to the COVID-19 pandemic, the university where the robot is located and this experiment was conducted is closed, as the health organizations recommend. Therefore, we were not allowed to perform more experiments like this to other goal-doors to show further the good performance of our semantic OS system in the physical world. 


\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/1_Real_Exploration_11.png}
         \caption{}
         \label{fig:realResults_aa}
     \end{subfigure}~~
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/2_Real_Exploration_22.png}
         \caption{}
         \label{fig:realResults_bb}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/3_Real_Exploration_33.png}
         \caption{}
         \label{fig:realResults_cc}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/4_Real_Exploration_44.png}
         \caption{}
         \label{fig:realResults_dd}
     \end{subfigure}
     \\[.5em]
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/5_Real_Exploration_55.png}
         \caption{}
         \label{fig:realResults_ee}
     \end{subfigure}~~ 
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/6_Real_Exploration_66.png}
         \caption{}
         \label{fig:realResults_ff}
     \end{subfigure}     
     \caption[Step-by-step of the performance of our NSOS system in the physical environment.]{\small Step-by-step of the performance of our NSOS system in the physical environment. \textit{a} shows the initial position, where the robot has started the searching, whereas \textit{f} shows the final step when the robot has found the goal-door 232.}
     \label{fig:realResults}
 \end{figure}

%\section{Related Work}
%\label{sec:relatedwork}
%The OS problem has been studied for many years in the robotics field. The proposed approaches range from multi-agent collaborating to search for an object~\cite{Ye1996Collaborative} to a single robot actively performing a semantic-based search~\cite{Zeng2020Semantic}. After many years subtopics of research arose within the OS, such as Indirect and Active Visual searches. Despite this long period in which new approaches have been proposed, no detailed surveys in the literature shed light on this latter subtopic. However, it is possible to find comprehensive surveys on wider topics such as salient objects detection~\cite{Borji2019Salient}, visual attention \cite{Begum2010Visual}, and as pointed out in~\cite{Aydemir2013Active}, active vision~\cite{Chen2011Active,Chung2011Search}. It is important to mention that even though it has not been proposed as a survey, in~\cite{Aydemir2013Active} Aydemir et al. presented a comprehensive review of some of the most important works related to OS. Hence, we review other works not presented in~\cite{Aydemir2013Active}, that are as important to the development of the work of this chapter as the presented ones. This review allows us to show how our work compares to the visual OS body of research. It also shows how our system contributes to the advancement of state-of-the-art.

%Aydemir and colleagues presented spatial representations and a different planner to the OS problem in a series of papers. In~\cite{Aydemir2011Search}, the authors proposed a spatial relation that describes topological relationships between objects. They used that description to create potential search actions for the OS problem since they aimed to relax the assumption that objects start within the robot's sensory reach. Their spatial representation was improved in~\cite{Aydemir2011Object}, where they proposed a combination of a 3D metric map, which supports obstacle avoidance and path planning, and a topological map called place map, which maintains the topology of the environment. The outcome of such combination was a conceptual map that connects symbols representing instance knowledge about the environment with spatial concepts such as objects, room categories, or appearances. The spatial relation introduced in~\cite{Aydemir2011Search} was used later in~\cite{Aydemir2011Plan} when they combined semantic cues to guide the object search process in a larger environment. A switching planner combines a continual classical planner that decides the overall strategy of the search, and a decision-theoretic planner, which uses a probabilistic sensing model to set the low-level observation actions. This same paper also proposed an exploration strategy that considers the OS task since its start without an initial environment map. The next proposal has argued that there is a strong correlation between local 3D structure and object placement, called 3D context. The authors argued in~\cite{Aydemir2012ExploitingAnd} that local 3D shape around objects is a strong indicator of the placement of these objects. Hence, they used a more general model to learn the relationship between 3D context and objects, in contrast to the correlation between objects and the environment's appearance. Their approach was evaluated considering a large RGB-D dataset, showing the effect of using 3D context in an object detection task. Besides making an RGB-D dataset publicly available in~\cite{Aydemir2012ExploitingAnd}, in~\cite{Aydemir2012What} Aydemir et al. also published a dataset called KTH. In this case, the dataset is composed of a set of floor plans that encompasses, in total, 37 buildings, 165 floors, and 6248 rooms. In addition to KTH, another contribution of their work was two methods for predicting indoor topologies and room categories given a partial map of the environment. The goal was to predict what lies ahead in the topology of the environment through its topology. Finally, in~\cite{Aydemir2013Active} the OS is performed without any initial map, and hence, besides performing the search, their approach explored the environment as well. One of their main contributions was the balance between exploration and exploitation, which makes the robot explore more regions of the environment only after carefully searching for the object in the known regions. Their proposed OS system reasons about whether exploit the known part or explore the unknown part, based on a model that describes the distribution over possible extensions to the current world.

%The idea of relying on significant and visible landmarks to narrow down the search was not found only in~\cite{Aydemir2012ExploitingAnd}. Zeng et al. exploited background knowledge about common spatial relationships between landmarks and target objects~\cite{Zeng2020Semantic}. Their proposal, called Semantic Linking Maps (SLiM), maintained the belief over the locations of the target object and the landmark. Simultaneously, it accounted for probabilistic inter-object spatial relations. In contrast to the 3D context-based OS systems, Rasouli et al. proposed an attention-based OS system~\cite{Rasouli2020Attention}. They argued that an OS system must be responsive, directive, spatiotemporal, and efficient, which are the characteristics addressed by their model. It embedded visual attention in an n-step decision-making algorithm formalized as a 1st-order Markov process. The use of visual attention increased the robot's awareness of the environment. Hence, they used all relevant available visual information, leveraging the spatial and appearance information about the object. Rasouli and Tsotsos also relied on visual attention methods to reduce computational costs on their robotic visual search~\cite{Rasouli2017Integrating}. They proposed a three-pronged probabilistic search algorithm that incorporated three forms of visual attention: viewpoint selection, saliency, and object-based models. On their model, attention is used to generate maps with highlighted areas in the image which are more likely to contain an object of interest. The experiments showed that the proposed three-tier attention framework decreased the search cost in terms of distance traveled, search time, and the number of actions taken. Saidi et al. explored a different robot than the other works that opted for wheeled robots since their OS system was proposed based on the specificities of a humanoid robot~\cite{Saidi2007Active}. A visibility map, which constrains the sensor parameter space, was used to avoid unnecessary calls to the rating function that evaluates the interest of a potential next view through the analysis of the theoretical field of view.

%It is worth mentioning that our semantic OS system also considers the exploration of unknown environments as part of the problem. We aim to perform OS in an entire unknown search space, which requires switching between the exploration of unknown regions and the exploitation in already known regions. Hence, it is important to present some works related to exploration. Here, they are divided into two significant groups regarding their goals. First, strategies that aim to explore the whole environment, usually finishing when the robot has visited the entire free area \cite{Quattrini2016ASemantically, Girdhar2014Curiosity}. Second, goal-directed strategies that aim to reach a goal, such as searching for an object, a room, or a person. 

%The papers reviewed in this section use semantic information to improve their findings. Some of them use a semantic map, whereas others use semantic properties from objects. The system proposed by Aydemir et al. \cite{Aydemir2013Active} focuses on a large-scale environment, where the robot should find objects using mainly visual sensing. They affirm that rather than performing an exhaustive search in the area, their system could find the object guiding the robot towards areas more likely to contain it. The probability is calculated considering extracted semantic cues from appearance, geometry, the topology of the environment, and general semantic knowledge of the indoor space. They showed that the results improved drastically after including a semantic description in their search system.

%Differently, the framework proposed by Veiga et al. \cite{Veiga2016Efficient} searches for objects in domestic environments. It is composed of a system that perceives the query object in RGB-D images through inference and sensor information. The outcome of this process, called knowledge, is saved and updated in a semantic map. Experiments in a realistic apartment have shown that their framework worked well in practice, presenting a reliable and efficient search approach. 

%Another significant work that searches objects in domestic scenarios is Rogers' et al. \cite{Rogers2013Robot}. In contrast to \cite{Veiga2016Efficient} that proposed a modular system, their approach considered the context of the environment. A graph, connecting places and objects within these places, is used to predict objects' presence (or absence) based on the room categories. The reasoning over the graph, combined with a planner, is used to perform an object search task. Experiments showed that the robot was able to find objects in the environment.

%Talbot et al. \cite{Talbot2016Find} and Schulz et al. \cite{Schulz2015Robot} proposed navigation approaches that are also Goal-Directed, despite not being exploration ones. The idea of an original and abstract map that links symbolic spatial information with observed symbolic information and actual places in the real world was firstly introduced by \cite{Schulz2015Robot}. This map is used to make inferences about the location of places. Later, Talbot et al. \cite{Talbot2016Find} extended the idea of the abstract maps, proposing a novel method that defines the topological structure and spatial layout information encoded in spatial language phrases. The system has shown to complete the task by traveling slightly further than the optimal path.

%Despite the good outcomes from the solutions presented by the papers mentioned above, there is still room for improvements. In~\cite{Aydemir2013Active} Aydemir et al. depended on prior semantic knowledge about indoor spaces obtained from databases. Talbot et al. \cite{Talbot2016Find} and Schulz et al.\cite{Schulz2015Robot} depended on a priori abstract maps. Veiga et al. \cite{Veiga2016Efficient} required beforehand information to learn about objects and the environment. Additionally, it used a 3D recognition-based framework from the Point Cloud Library (PCL) for object recognition, which is computationally expensive. Rogers et al. \cite{Rogers2013Robot} also implemented PCL to segment data from RGB-D sensor, continuing to cluster the points, which is a heavy workload for computers. It is also important to highlight that none of them has explored the benefits of textual information available in the environment. 

%Our proposed OS system reads the door sign numbers through an efficient computer vision algorithm and analyses them to decide whether the current path is promising for the robot to find the goal-door. It does not require an environment description or other instruction in advance, suitable for tasks in unknown environments. Additionally, it is not computationally expensive, and a simple computer and two cameras embedded in a robot can execute it. Relying on textual information from the environment and inferring semantic information is another contribution of our work compared to the other papers reviewed here. Given that no information or map is necessary for our system, it is a good solution for entirely unknown environments. %We understand that some buildings are labelled with a configuration that contains both numbers and letters, which is not supported by our system. However, 

\section{Summary} 
\label{sec:conclusion}
We proposed NSOS, a semantic OS system that relies on organisational semantic information inferred from numbers within the environment. The proposed system aims to demonstrate that it is possible to take advantage of numbers from textual signs to estimate the organisation of the environment and improve the robot's performance. Even though we have not tested our system with traffic signs or outdoor advertisements, for example, and only with door signs, the results show that detecting complementary information from signs is promising for robotic solutions. Besides, our NSOS system also intends to encourage the mobile robotics research community to explore the advantages of semantic information for mobile robot tasks. To the best of our knowledge, there is no other OS system in the literature that extract semantic information from any of the aforementioned examples. Hence, we could not present a fair comparison between our proposal and other published method, other than the comparison we did with the Greedy OS sytem.

The main contributions of this chapter are: 
\begin{itemize}
    \item a robust semantic planner, based on five different factors, that reason over the door signs to find the goal-door with the robot traveling the shortest possible distance;
    \item a semantic OS system called NSOS which, by using our semantic planner, can reason over the door signs and estimate the organisation of the environment and make the robot avoid continuing searching on non-promising regions;
    \item an analysis of the usage of information inferred from numbers as input to the semantic planner within our NSOS. In general, the analysis shows that NSOS presented better results than human participants, both in the same simulation setup.
\end{itemize}

Our semantic planner applied to the OS system presented an excellent performance, mainly when compared to the results from the greedy system and humans performing the searching. Besides providing the shortest distance traveled, and by consequence, it was also the fastest search system, given that the robot moved with the same velocity in all the experiments. It is important to mention that no experiments were conducted in an environment where its rooms were randomly signed. This is because we believe that in such kind of environment, probably not even humans would be able to rely on the random signs and efficiently search for the target door sign, and our NSOS system would rely on an input that is not reliable. On the other hand, despite not being random, the \textit{Hotel} map presents very challenging door sign configurations. Some corridors have a door sign sequence that does not increase or decrease, which does not have a predominant parity. These conditions do not reflect a well-structured environment, but our proposal still presented a robust performance, with better results than the other tested approach and humans. Our NSOS system does not support door signs labeled with other characters other than numbers. However, this limitation could be overcomed with the development of different heuristics (or factors) that model the organisation of other labeling patterns. The modular characteristic of our semantic planner from NSOS allows the replacement of factors by new ones designed for interpreting specific labels. All the other parts from NSOS could remain the same.

Our semantic planner does not require that the door signs of the environment are set according to a specific pattern, as confirmed by the wide variety of the four tested scenarios. The scenario \textit{Hotel} demonstrates how different the door signs can be located, and according to the feedback from the participants after their participation, the \textit{Hotel} is indeed a little bit confusing for them. 

The results show that our NSOS system presented better or similar results than the ones from human participants. However, it is important to highlight that these results were obtained when humans piloted the robot in the same simulation setup as the other experiments. The human eyes have a wider field of view than cameras, so in this case, it would be unfair to compare the performance of humans against robots if they had different visual sensors. That is why only human reasoning was considered in the experiments of this chapter.

%Finally, as future work, \textcolor{black}{we aim to perform more experiments with the physical robot in real world to measure the performance of our proposal in different scenarios. We also intend to make our code publicly available, as it is implemented based on the Robot Operating System (ROS), and hence, it can be easily reused by the research community. The same applies to the door sign simulator developed by us for testing our proposal. About the type of the door sign, we also aim to explore other standards that also include letters, not only numbers. This achievement would make our proposal suitable for applications in a broader range of environments.} Besides, the potential of machine learning could be applied to the object searching problem. A machine learning-based system could learn how the door signs are set within the environment instead of modeling the growth factor of a sequence or the odd and even factors. In addition to that, investigating the other gains of textual information, such as reading the traffic signs to improve the driving performance of autonomous cars, is another topic that should be investigated. \textcolor{black}{Lastly, our approach depends on the map segmentation, which is done by KDE, to group the detected door signs. In KDE, the kernel size changes the total amount and the area of the segments, besides being one of the parameters of our proposal. Therefore, the dependency of this parameter by our proposal should be investigated to make it more robust and stable.} 