\chapter{Theoretical Background}
In the previous chapter, we have argued that multiple robotic tasks would benefit from exploiting the semantic information inferred from everyday environments that surrounds the robot. We have chosen the object search (OS) problem to explore this idea, which aims to estimate a target object's location in a large unknown environment, usually with a camera attached to a mobile robot. We believe investigating this problem can expand our understanding regarding the benefits of employing semantic information to expand the robot's perception. 

This chapter presents a theoretical background detailing techniques used throughout this thesis. The OS problem requires the robot to map the unknown environment and to estimate its position simultaneously. SLAM systems fulfill these requirements, as it computes the state estimation and builds an environment representation. Hence, we address the basic concepts of such systems and mobile robotics in general, from the individual localization and mapping problems to how they are combined into the SLAM systems. Besides, we cover the generic and central formulation of OS problems, which is the basis for the works presented in Chapters X and Y [TO DO].

\section{The Basics of Mobile Robotics}
\label{sec:chap2_basic_mobile_robotics}
Mobile robots perform several tasks that require them to be aware of their positions in the environment and obstacles' positions to avoid collisions. In most realistic scenarios where the robots are deployed, such information is not directly available. Hence, the robots have to estimate it with their sensors, which provide noisy and partial data from the environment~\cite{Thrun2006Probabilistic}.

The state estimation in mobile robotics can be summarized in four variables: 
\begin{itemize}
	\item $\bs{x}_t$: robot's pose at time step $t$. It is composed by a three dimensional vector containing $(x, y, \theta)^T$, in which $x,y$  represent the position and $\theta$ the orientation. A sequence of robot's poses from time step $0$ to time step $t$ is defined as ${\bs{x}_{0:t}= \{ \bs{x}_0, \bs{x}_1, \cdots, \bs{x}_t\}}$.
	\item $\bs{m}_i$: object $i$'s position in the environment. A list of $N$ objects, with $1 \leq n \leq N$, in the environment along with their properties is given by the vector $\bs{m} = (\bs{m}_1, \bs{m}_2, \cdots, \bs{m}_N)^T$.
	\item $\bs{u}_t$: control data at instant $t$, and it corresponds to the change of state in the time interval $(t - 1;t]$. The sequence of control data that takes the robot from the initial position to $\bs{x}_t$ is denoted by ${\bs{u}_{1:t} = \{\bs{u}_1, \bs{u}_2, \cdots, \bs{u}_t\}}$.
	\item $\bs{z}^i_t$: the $i$-th measurement made by the robot at instant $t$. The vector of all of them acquired at the same instant $t$ is $\bs{z}_t = (\bs{z}^1_t, \bs{z}^2_t, \cdots, \bs{z}^K_t)^T$, whereas $\bs{z}_{1:t} = \{\bs{z}_1, \bs{z}_2, \cdots, \bs{z}_t\}$ expresses the history of all observations.	
\end{itemize}

\begin{figure}[ht!]
    \footnotesize
    \centering
    \begin{subfigure}[b]{0.7\columnwidth} 
        \includegraphics[width=\textwidth]{figs/caption_example.pdf} %\caption{SLAM} 
    \end{subfigure}\\[0.1cm]
    \begin{subfigure}[b]{0.31\columnwidth} 
        \includegraphics[width=\textwidth]{figs/localization_example.pdf} \caption{Localization} 
        \label{chp02_fig:problems_robotics_localization}
    \end{subfigure}~ 
    \begin{subfigure}[b]{0.31\columnwidth} 
        \includegraphics[width=\textwidth]{figs/mapping_example.pdf} \caption{Mapping}
        \label{chp02_fig:problems_robotics_mapping}
    \end{subfigure}~
    \begin{subfigure}[b]{0.31\columnwidth} 
        \includegraphics[width=\textwidth]{figs/slam_example.pdf} \caption{SLAM} 
        \label{chp02_fig:problems_robotics_slam}
    \end{subfigure}\\[0.1cm]
    \caption[Fundamental problems in mobile robots and their state estimation.]{Fundamental problems in mobile robots and their state estimation. It is estimated: (a) robot's pose, (b) map, (c) both of them simultaneously. Extracted from~\cite{Maffei2017Translating}.}
    \label{chp02_fig:problems_robotics}
\end{figure}  

After defining the four variables that are the basic foundation for state estimation in mobile robotics, it is worthing to explain their role in different estimation problems.  The set of controls $\bs{u}_{1:t}$ and measurements $\bs{z}_{1:t}$ are always known since the robot's sensors provide them. Inertial measurement units and wheel encoders are examples of sensors that provide control data, whereas lidars, sonars, and cameras measure the environment. The other two variables, robot's pose, $\bs{x}_{0:t}$, and environmental map, $\bs{m}$, are not necessarily known. Depending on the estimation problem, it is necessary to estimate different variables, like the three examples depicted in Fig.~\ref{chp02_fig:problems_robotics}. In \textit{Localization}, Fig.~\ref{chp02_fig:problems_robotics_localization}, the map is known in advance, and hence, only the robot's pose is estimated. The opposite happens in \textit{Mapping}, Fig.~\ref{chp02_fig:problems_robotics_mapping}, as the map is built based on the known robot's pose. Lastly, in \textit{SLAM}, Fig.~\ref{chp02_fig:problems_robotics_slam}, which combines the two previous problems, none of them is given a priori, and therefore, both are estimated simultaneously. 

\begin{figure}[ht!]
    \footnotesize
    \centering
    \begin{subfigure}[b]{\columnwidth} 
        \includegraphics[width=\textwidth]{figs/graphical_models_loc_map_slam.pdf} %\caption{SLAM} 
    \end{subfigure}
    \caption[Graphical model of the fundamental mobile robotics problems.]{Graphical model of the fundamental mobile robotics problems: localization, mapping, and two SLAM variations. Adapted from~\cite{Maffei2017Translating}.}
    \label{chp02_fig:state_estimations}
\end{figure}  

Localization is the most basic perceptual problem in robotics. It aims to determine the robot's pose relative to a given map of the environment. Localization can also be seen as a problem of coordinate transformation, in which it is established a correspondence between the map coordinate system and the robot's local coordinate system~\cite{Thrun2006Probabilistic}.  There are multiple localization problems, and they are not equal in terms of their difficulty level. One characteristic that divides this problem into local and global localization is the awareness of the robot's initial pose. The former assumes that the initial robot's pose is known. Therefore, the problem becomes a sort of position tracking in which the noise in the measurements is adjusted in robot motion, commonly by a Gaussian distribution. On the other hand, the latter is unaware of the initial pose, making it perform the localization globally (where the name comes from) in the map. The global localization has a higher difficulty level than the local one, but one of its variations is even more challenging, called the kidnapped robot problem. It addresses the problem of a localized robot being teleported to some other location in that the robot might believe it knows where it is while it does not.  Although a robot is rarely kidnapped in practice, recovering from localization failures is essential for autonomous robots. 

The formulation of the global localization problem is presented in Fig.~\ref{chp02_fig:state_estimations}, which depicts a few iterations of the robot's pose estimation and how the variables are used. The map $\bs{m}$ is already known, whereas the $\bs{x}_{0:t}$ must be estimated based on the controls $\bs{u}_{1:t}$ and the measurements $\bs{z}_{1:t}$. For the case of local localization, the $\bs{x}_0$ is known and hence, does not need to be estimated. Markov localization is a probabilistic algorithm that addresses all the localization problems mentioned earlier. It applies the Bayes filter, $p(\bs{x}_t \mid \bs{u}_{1:t}, \bs{z}_{1:t}, \bs{m})$, to transform a probabilistic belief at time $t-1$ into a belief at time $t$.

Many other localization algorithms implement Markov localization in mobile robotics. Three of them have been in the spotlight for a long time and are prevalent in this field: Kalman filter, grid-based filter, and particle filter. The former filters and predicts in linear dynamics and measurement functions~\cite{Leonard1991Mobile}, whereas the grid-based filter approximates the estimations by decomposing the state space into finitely many regions of the grid map~\cite{Burgard1998Integrating}. The key idea of the latter, particle filter, is to represent the estimation by a set of random state samples, called particles, drawn from the previous estimation. It can represent a much broader space of distribution, in contrast to the Kalman filter that is more strict to Gaussians~\cite{Dellaert1999Monte}. The particle filter implementation for mobile robotics is also known as Monte Carlo Localization (MCL), widely used in many different robotics applications for multiple robot types~\cite{Dellaert1999Monte}. 

Mapping, for the case of the robot's poses are known, is the problem of generating consistent maps from noisy and imprecise measurement data~\cite{Thrun2006Probabilistic}. The estimated belief of the map, $p(\bs{m} \mid \bs{x}_{1:t}, \bs{z}_{1:t})$, considers the set of all measurements up to time $t$, $\bs{z}_{1:t}$, along with the robot's path defined by its history of all poses, $\bs{x}_{1:t}$, as shown in Fig.~\ref{chp02_fig:state_estimations}. Comparing the graphical models of the localization and mapping problems in Fig.~\ref{chp02_fig:state_estimations}, one can say that they are opposite each other in terms of which variable each estimates. This thought makes sense, since whereas the former relies on $\bs{m}$ to estimate $\bs{x}_{0:t}$, the latter relies on $\bs{x}_{0:t}$ to estimate $\bs{m}$. It is important to mention that the controls $\bs{u}_{1:t}$ play no role in this context, as the path is already known. Besides, the robot's initial pose $\bs{x}_0$ is omitted from the map estimation because no measures are taken when the robot is at that pose.

Similar to the localization problem that groups multiple localization types, the mapping problem also represents a general idea implemented by different map types. The feature-based maps represent the cartesian location of features, which are distinct objects in the physical world, extracted from the measurements, such as images from visual sensors or a vector of distances from a 2D lidar~\cite{Salas2013Slam++, Engel2014Lsd, Mur2015Orb}. The advantage of such a map type is the reduction of computational complexity, as the feature space has a lower dimension than the raw measurement. For example, the eight 3D edges of a boudingbox encircling a car are computationally cheaper to process than a point cloud from a 3D lidar. Another map type within the mapping problem is called location-based. It represents in each map component $\bs{m}_i$ the regions from the environment, regardless of whether they contain objects. This way, any location in the world has a label on the map, not only features. Occupancy grid maps are often considered the most popular location-based map~\cite{Thrun2006Probabilistic}. They discretize the environment into small portions called grid cells, which store information about the area it covers. In general, this information in each cell is a single value representing the probability that an obstacle occupies this cell. The size of the cells defines the map resolution, which brings a tradeoff between the level of details and the demand for memory resources. 

Lastly, Simultaneous localization and mapping (SLAM), also known as Concurrent Mapping and Localization, is undoubtedly the most fundamental and challenging problem in robotics~\cite{Thrun2006Probabilistic}. SLAM problems appear in scenarios where the environmental map is unavailable and the robot is unaware of its pose. In contrast to the other two problems presented earlier, which have to estimate either the map $\bs{m}$ or $\bs{x}_{1:t}$, in SLAM problems, the robot has to perform the estimation of both variables at the same time, as shown in Fig.~\ref{chp02_fig:state_estimations}. Since the robot does not know its pose and there is no map, the pose $\bs{x}_0$ is assumed, by convention, to be $(0,0,0)^T$. The high difficulty level of SLAM comes from the double dependency of localization and mapping: to estimate the pose, the robot needs a map from the environment, whereas to estimate the map, the robot need to know its pose. 

The SLAM problem is divided into two forms based on what is estimated: online and full SLAMs. The former focus on estimating only the posterior over the current robot's pose $\bs{x}_t$ and the map $\bs{m}$, $p(\bs{x}_t, \bs{m} \mid \bs{z}_{1:t}, \bs{u}_{1:t})$. The full SLAM computes the same estimation, but with the entire robot's trajectory $\bs{x}_{1:t}$ along with the map $\bs{m}$, $p(\bs{x}_{1:t}, \bs{m} \mid \bs{z}_{1:t}, \bs{u}_{1:t})$.

The majority of the algorithms for the online SLAM problem are incremental, i.e., the idea is to estimate the posterior probability on the current robot state and map as the robot moves, discarding past measurements and controls once they have been processed. The Kalman and particle filters are also used in this context, besides the localization one as previously discussed. The Extended Kalman Filter is the basis of one of the earliest online SLAM approaches, linearizing motion and observation models, which usually are nonlinear, to perform the online SLAM estimations~\cite{Maffei2017Translating}. An online SLAM problem that is based on particle filter is known as Rao-Blackwellized particle filter (RBPF)~\cite{Murphy1999Bayesian, Doucet2000Rao, Grisettiyz2005Improving, Grisetti2007Improved}. In RBPF, each particle carries an individual grid map of the environment, representing a hypothesis of the robot's trajectory.  The number of particles is directly related to the map quality since the higher this number, the broader is the hypotheses variety. However, there is a cost associated with each particle, and hence,   it is not practical to increase the number of particles until the estimated map matches the physical world. 

The algorithms for the full SLAM problem calculate a posterior over the entire path, which solves an issue in the online SLAM problem. Discarding the previous states after estimating the current one, also known as Markov assumption, implies that the possible poor estimations in the past are not adjustable. In contrast, the full SLAM problems backpropagate to the previous estimations the error reduction computed in the current state calculation. GraphSLAM captures the essence of the full SLAM problem, since it calculates a solution for the offline problem over $\bs{x}_{1:t}$ and $\bs{z}_{1:t}$ in $\bs{m}$. Despite the advantage of improving previous state estimations, full SLAM algorithms are computationally heavy due to the optimization of nonlinear quadratic constraints. 

Explaining the fundamental problems of mobile robotics, from the simplest localization to the more complex SLAM problems, helps to understand why the OS works for unknown environments depend on a SLAM system. Since our works presented in the following chapters are designed for similar conditions (large and unknown environments), we opted to rely on GMapping~\cite{Grisetti2007Improved}. It is an online SLAM algorithm based on RBPF that provides a 2D grid map, and each cell contains a value that means whether the region it represents is unknown (to the SLAM system), occupied (obstacle), or free.

\section{OS problem formulation}
The OS problem aims to find an efficient strategy for localizing a target object in a large unknown indoor environment. Since our works presented in this thesis are based on a 2D grid map, the search strategies from these works reason over the map $\bs{m}$, and they decide what cell $c$ is currently more promising to localize the target object while minimizing the total cost. We define cost as the distance traveled by the robot during the search, as the longer the robot's path, the higher is the amount of resources (battery and time) it spends. The robot is equipped with a 2D lidar to build the grid map and a camera used to gather visual cues for semantic information estimation. Both sensors are fixed to the robot's body, and hence, we consider only the movements performed by a ground mobile robot.  

Additionally, let $\Psi(c)$ be the probability distribution for a map cell, $c$, where the target object is in $\bs{m}$. Depending on the level of a priori knowledge of $\bs{m}$ and $\Psi(c)$, it is possible to address the OS problem in three different ways: 
\begin{itemize}
	\item $\bs{m}$ \textbf{and} $\Psi(c)$ \textbf{are known}: the problem becomes a sensor placement,  aiming to reduce the search cost by moving the robot straight to the cell $c$.
	\item $\bs{m}$ \textbf{is known}: in case the map is available a priori (or acquired through a separate mapping step), the mobile robot should either rely on a generic probability distribution or move through the environment to gather information. The inspection performed by the robot is to get information about the objects and update the probability distribution. 
	\item $\bs{m}$ \textbf{and} $\Psi(c)$ \textbf{are unknown}: the robot needs to map the environment with the aid of a SLAM system, at the same time that it collects information to compute the probability distribution. Since the robot performs OS in an unknown environment, it has to tradeoff between expanding the mapped area and executing sensing actions to search for the target object carefully. This scenario is also known ad the exploration vs. exploitation problem. 
\end{itemize}

In this thesis, both the second and third points are considered, addressed individually in different works, in chapters [REF CHAPTERS]. In general, each of these works has a semantic search strategy, i.e., it incorporates semantic information into the estimations to improve the performance. However, it is important to mention that these semantic search strategies consider common-sense knowledge, which is not environment-specific, and integrate high-level human concepts. In the context of this thesis, common-sense knowledge encodes semantic information inferred from text signs and objects' placement over a while. Such information is valuable for our works because it reduces the search space and improves the search for a human-like performance.