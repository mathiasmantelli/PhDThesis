\chapter{Theoretical Background}
In the previous chapter, we have argued that multiple robotic tasks would benefit from exploiting the semantic information inferred from everyday environments that surrounds the robot. We have chosen the object search (OS) problem to explore this idea, which aims to estimate a target object's location in a large unknown environment, usually with a camera attached to a mobile robot. We believe investigating this problem can expand our understanding regarding the benefits of employing semantic information to improve the robot's perception. 

This chapter presents a theoretical background detailing techniques used throughout this thesis. The OS problem requires the robot to map the unknown environment and to estimate its position simultaneously. SLAM systems fulfill these requirements, and hence, we address the basic concepts of such systems and other basic concepts in mobile robotics. Besides, we cover the generic and central formulation of OS problems, which is the basis for the works presented in Chapters X and Y [TO DO].

\section{The Basics of Mobile Robotics}
Mobile robots perform several tasks that require them to be aware of their positions in the environment and obstacles' positions to avoid collisions. In most realistic scenarios where the robots are deployed, such information is not directly available. Hence, the robots have to estimate it with their sensors, which provide noisy and partial data from the environment (CITE PROB. ROBOTICS).

The state estimation in mobile robotics can be summarized in four variables: 
\begin{itemize}
	\item $\bs{x}_t$: robot's pose at time step $t$. It is composed by a three dimensional vector containing $(x, y, \theta)^T$, in which $x,y$  represent the position and $\theta$ the orientation. A sequence of robot's poses from time step $0$ to time step $t$ is defined as $\bs{x}_{0:t}= \{ \bs{x}_0, \bs{x}_1, \cdots, \bs{x}_t\}$.
	\item $\bs{m}_i$: object $i$'s position in the environment. A list of $N$ objects, with $1 \leq n \leq N$, in the environment along with their properties is given by the vector $\bs{m} = (\bs{m}_1, \bs{m}_2, \cdots, \bs{m}_N)^T$.
	\item $\bs{u}_t$: control data at instant $t$, and it corresponds to the change of state in the time interval $(t - 1;t]$. The sequence of control data that takes the robot from the initial position to $\bs{x}_t$ is denoted by $\bs{u}_{1:t} = \{\bs{u}_1, \bs{u}_2, \cdots, \bs{u}_t\}$.
	\item $\bs{z}^i_t$: the $i$-th measurement made by the robot at instant $t$. The vector of all of them acquired at the same instant $t$ is $\bs{z}_t = (\bs{z}^1_t, \bs{z}^2_t, \cdots, \bs{z}^K_t)^T$, whereas $\bs{z}_{1:t} = \{\bs{z}_1, \bs{z}_2, \cdots, \bs{z}_t\}$ expresses the history of all observations.	
\end{itemize}

After defining the four variables that are the basic foundation for state estimation in mobile robotics, it is worthing to explain their role in different estimation problems.  The set of controls $\bs{u}_{1:t}$ and measurements $\bs{z}_{1:t}$ are always known since the robot's sensor provides them. Inertial measurement units and wheel encoders are sensors that provide control data, whereas lidars, sonars, and cameras measure the environment. The other two variables, robot's pose $\bs{x}_{0:t}$ and environmental map $\bs{m}$, are not necessarily known. Depending on the estimation problem, it is necessary to estimate different variables, like the three examples depicted in Fig. [REF THE FIG]. In \textit{Localization}, the map is known in advance, and hence, only the robot's pose is estimated. The opposite happens in \textit{Mapping}, as the map is built based on the robot's pose. Lastly, in \textit{SLAM}, which combines the two previous problems, none of them is given a priori, and therefore, both are estimated simultaneously. 

Localization is the most basic perceptual problem in robotics. It aims to determine the robot's pose relative to a given map of the environment. Localization can also be seen as a problem of coordinate transformation, in which it is established a correspondence between the map coordinate system and the robot's local coordinate system. (CITE PROB. ROBOTICS).  There are multiple localization problems, and not each of them is equally difficult. One characteristic that divides this problem into local and global localization is the awareness of the robot's initial pose. The former assumes that the initial robot's pose is known. Therefore, the problem becomes a sort of position tracking in which the noise is adjusted in robot motion commonly by a Gaussian distribution. On the other hand, the latter is unaware of the initial pose, making it perform the localization globally (where the name comes from) in the map. The global localization has a higher difficulty level than the local one, but one of its variations is even more challenging, called the kidnapped robot problem. It addresses the problem of a localized robot being teleported to some other location in that the robot might believe it knows where it is while it does not.  Although a robot is rarely kidnapped in practice, recovering from localization failures is essential for autonomous robots. 

The formulation of the global localization problem is presented in Figure [REF THE FIG], which depicts a few iterations of the robot's pose estimation and how the variables are used. The map $\bs{m} = (\bs{m}_1, \bs{m}_2, \bs{m}_3, \bs{m}_4)^T$ is already known, whereas the $\bs{x}_{0:t}$ must be estimated based on the controls $\bs{u}_{1:t}$ and the measurements $\bs{z}_{1:t}$. For the case of local localization, the $\bs{x}_0$ is known and hence, does not need to be estimated. Markov localization is a probabilistic algorithm that addresses all the localization problems mentioned earlier. It applies the Bayes filter, $p(\bs{x}_t \mid \bs{u}_{1:t}, \bs{z}_{1:t}, \bs{m})$, to transform a probabilistic belief at time $t-1$ into a belief at time $t$.

Many other localization algorithms implement Markov localization in mobile robotics. Three of them have been in the spotlight for a long time and are prevalent in this field: Kalman filter, grid-based filter, and particle filter. The former filters and predicts in linear dynamics and measurement functions (CITE KALMAN), whereas the grid-based filter approximates the estimations by decomposing the state space into finitely many regions of the grid map (CITE GRID). The key idea of the latter, particle filter, is to represent the estimation by a set of random state samples, called particles, drawn from the previous estimation. It can represent a much broader space of distribution, in contrast to the Kalman filter that is more strict to Gaussians (CITE PARTICLE). The particle filter implementation for mobile robotics is also known as Monte Carlo Localization (MCL), widely used in many different robotics applications for multiple robot types. 

Mapping, for the case of the robot's poses are known, is the problem of generating consistent maps from noisy and imprecise measurement data (CITE PROB ROB). The estimated belief of the map, $p(\bs{m} \mid \bs{x}_{1:t}, \bs{z}_{1:t})$, considers the set of all measurements up to time $t$, $\bs{z}_{1:t}$, along with the robot's path defined by its history of all poses, $\bs{x}_{1:t}$, as shown in Fig [REF FIG]. Comparing the graphical models of the localization and mapping problems, [REF FIGS], one can say that they are opposite each other in terms of which variable each estimates. This thought makes sense, since whereas the former relies on $\bs{m}$ to estimate $\bs{x}_{0:t}$, the latter relies on $\bs{x}_{0:t}$ to estimate $\bs{m}$. It is important to mention that the controls $\bs{u}_{1:t}$ play no role in this context, as the path is already known. Besides, the robot's initial pose $\bs{x}_0$ is omitted from the map estimation because no measures are taken when the robot is at that pose.

Similar to the localization problem that groups multiple localization types, the mapping problem also represents a general idea implemented by different map types. The feature-based maps represent the cartesian location of features, which are distinct objects in the physical world, extracted from the measurements, such as (CITE EXAMPLES images from visual sensors or a vector of distances from a 2D lidar.). The advantage of such a map type is the reduction of computational complexity, as the feature space has a lower dimension than the raw measurement. For example, the eight 3D edges of a boudingbox encircling a car are computationally cheaper to process than a point cloud from a 3D lidar. Another map type within the mapping problem is called location-based. It represents in each map component $\bs{m}_i$ the regions from the environment, regardless of whether they contain objects. This way, any location in the world has a label on the map, not only features. Occupancy grid maps are often considered the most popular location-based map (CITE PROB ROB). They discretize the environment into small portions called grid cells, which store information about the area it covers. In general, this information in each cell is a single value representing the probability that an obstacle occupies this cell. The size of the cells defines the map resolution, which brings a tradeoff between the level of details and the demand for memory resources. 